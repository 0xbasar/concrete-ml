{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48790992",
   "metadata": {},
   "source": [
    "# MNIST Classification with Brevitas Quantization-Aware Training \n",
    "\n",
    "In this notebook, we show how to use [Brevitas](https://github.com/Xilinx/brevitas) to perform quantization-aware training on the famous [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, using a fast FHE-friendly model with a low bit-width accumulator of 6-bits. Then, we show how the Virtual Library can be used to finetune parameters and find the best setting, before finally checking computations in FHE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01d93a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Concrete-Numpy\n",
    "from concrete.numpy.compilation import Configuration\n",
    "\n",
    "# The QAT model\n",
    "from model import MNISTQATModel  # pylint: disable=no-name-in-module\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Concrete-ML\n",
    "from concrete.ml.torch.compile import compile_brevitas_qat_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9878260c",
   "metadata": {},
   "source": [
    "Here are the classical functions to train and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95137753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, criterion):\n",
    "    \"\"\"Train the model.\"\"\"\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data).squeeze()\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 4 == 0 and batch_idx % 500 == 0:\n",
    "            print(\n",
    "                f\"Train Epoch: {epoch + 1} [{batch_idx}/{len(train_loader.dataset) // len(data)}\"\n",
    "                f\" ({100.0 * batch_idx / len(train_loader):.0f}%)]{'':5}\"\n",
    "                f\"\\tLoss: {loss.item():.6f}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f432e903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, epoch, criterion):\n",
    "    \"\"\"Test the model.\"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader, disable=epoch % 4 != 0):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data).squeeze()\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    if epoch % 4 == 0:\n",
    "        print(\n",
    "            f\"Test set: Average loss: {test_loss:.4f}, \"\n",
    "            \"Accuracy: \"\n",
    "            f\"{correct}/{len(test_loader.dataset)} \"\n",
    "            f\"({100.0 * correct / len(test_loader.dataset):.0f}%)\"\n",
    "        )\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67d9b236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_dataset(train_kwargs, test_kwargs):\n",
    "    \"\"\"Get training and test parts of MNIST dataset.\"\"\"\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            transforms.Lambda(torch.flatten),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Manage datasets\n",
    "    dataset1 = datasets.MNIST(\"./data\", train=True, download=True, transform=transform)\n",
    "    dataset2 = datasets.MNIST(\"./data\", train=False, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bd35b2",
   "metadata": {},
   "source": [
    "The following function is used to test the model, it can be done either with the Virtual Library (i.e., without encryption) or in FHE. \n",
    "\n",
    "Checking things in VL is a very good habit to have within Concrete-ML since it allows to see the effect of the quantization on the accuracy, as well as to check the maximal bit-width of intermediate values, without having to wait as long as when things are computed in FHE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24132f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_test(\n",
    "    model,\n",
    "    use_virtual_lib,\n",
    "    test_data,\n",
    "    test_data_length,\n",
    "    test_target,\n",
    "    show_mlir,\n",
    "    current_index,\n",
    "):\n",
    "    # Compile the QAT model and test\n",
    "    configuration = Configuration(\n",
    "        enable_unsafe_features=True,  # This is for our tests only, never use that in prod\n",
    "        use_insecure_key_cache=True,  # This is for our tests only, never use that in prod\n",
    "        insecure_key_cache_location=\"/tmp/keycache\",\n",
    "    )\n",
    "\n",
    "    if use_virtual_lib:\n",
    "        print(f\"\\n{current_index}. Compiling with the Virtual Library\")\n",
    "    else:\n",
    "        print(f\"\\n{current_index}. Compiling in FHE\")\n",
    "\n",
    "    q_module = compile_brevitas_qat_model(\n",
    "        model,\n",
    "        test_data,\n",
    "        configuration=configuration,\n",
    "        show_mlir=show_mlir,\n",
    "    )\n",
    "\n",
    "    # Check max bit-width\n",
    "    max_bit_width = q_module.fhe_circuit.graph.maximum_integer_bit_width()\n",
    "\n",
    "    if max_bit_width > 8:\n",
    "        raise Exception(\n",
    "            f\"Too large bit-width ({max_bit_width}): training this network resulted in an \"\n",
    "            \"accumulator size that is too large. Possible solutions are:\"\n",
    "            \"    - this network should, on average, have 8bit accumulators. In your case an unlucky\"\n",
    "            f\"initialization resulted in {max_bit_width} accumulators. You can try to train the \"\n",
    "            \"network again\"\n",
    "            \"    - reduce the sparsity to reduce the number of active neuron connexions\"\n",
    "            \"    - if the weight and activation bitwidth is more than 2, you can try to reduce one \"\n",
    "            \"or both to a lower value\"\n",
    "        )\n",
    "\n",
    "    # Check the accuracy\n",
    "    if use_virtual_lib:\n",
    "        print(\n",
    "            f\"\\n{current_index + 1}. Checking accuracy with the Virtual Library \"\n",
    "            f\"(length {test_data_length})\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"\\n{current_index + 1}. Checking accuracy in FHE (length {test_data_length})\")\n",
    "\n",
    "    # Key generation\n",
    "    if not use_virtual_lib:\n",
    "        q_module.fhe_circuit.keygen()\n",
    "\n",
    "    correct_fhe = 0\n",
    "\n",
    "    # Reduce the test data, since very slow in FHE\n",
    "    reduced_test_data = test_data[0:test_data_length, :]\n",
    "    test_target = test_target[0:test_data_length, :]\n",
    "\n",
    "    q_reduced_data = q_module.quantize_input(reduced_test_data)\n",
    "    prediction = q_module.forward_in_fhe(q_reduced_data, simulate=use_virtual_lib)\n",
    "    prediction = q_module.dequantize_output(prediction)\n",
    "\n",
    "    correct_fhe = (np.argmax(prediction, axis=1) == test_target.ravel()).sum()\n",
    "\n",
    "    # Final accuracy\n",
    "    return correct_fhe, reduced_test_data.shape[0], max_bit_width"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a39bbb",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "Here, the user can change some settings. The most important ones are:\n",
    "- epochs: how many epochs during the training\n",
    "- sparsity: to define the number of active neurons in layers; make this value smaller and there will be less active neurons\n",
    "- quantization_bits: the number of bits during quantization. The larger the more accurate, but also the faster we are above the limits of Concrete-ML in term of maximal bitwidth\n",
    "- do_training: whether we do the training. If not, we use the previously saved ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb62ea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options: the most important ones\n",
    "epochs = 20\n",
    "sparsity = 4\n",
    "quantization_bits = 2\n",
    "do_training = True\n",
    "\n",
    "# Options: can be changed\n",
    "lr = 0.02\n",
    "gamma = 0.33\n",
    "test_data_length_reduced = 2  # This is notably the length of the computation in FHE\n",
    "test_data_length_full = 10000\n",
    "\n",
    "# Options: no real reason to change\n",
    "show_mlir = False\n",
    "batch_size = 32\n",
    "test_batch_size = 32\n",
    "use_cuda_if_available = True\n",
    "seed = None\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267c0c5e",
   "metadata": {},
   "source": [
    "Seeding if we want to, to try to make everything as reproducible as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ddb9ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using seed 4013218716\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seeding\n",
    "if seed is None:\n",
    "    seed = np.random.randint(0, 2**32 - 1)\n",
    "\n",
    "print(f\"\\nUsing seed {seed}\\n\")\n",
    "torch.manual_seed(seed);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcd103e",
   "metadata": {},
   "source": [
    "Settings few things for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e14bdea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using cpu device\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training and test arguments\n",
    "train_kwargs = {\"batch_size\": batch_size}\n",
    "test_kwargs = {\"batch_size\": test_batch_size}\n",
    "\n",
    "# Cuda management\n",
    "use_cuda = torch.cuda.is_available() and use_cuda_if_available\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "if use_cuda:\n",
    "    cuda_kwargs = {\"num_workers\": 1, \"pin_memory\": True, \"shuffle\": True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "print(f\"\\nUsing {device} device\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c0a2e7",
   "metadata": {},
   "source": [
    "Managing the MNIST data set, and splitting it into a train and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6c83939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manage dataset\n",
    "train_loader, test_loader = manage_dataset(train_kwargs, test_kwargs)\n",
    "img_size = train_loader.dataset.data[0].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caa0263",
   "metadata": {},
   "source": [
    "### Defining the model\n",
    "\n",
    "The model is defined in `model.py`. You may want to have a look to this file to see how things work in Brevitas or apply few changes here, to see what are the effects on the QAT training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e02f7dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "model = MNISTQATModel(quantization_bits, quantization_bits)\n",
    "model = model.to(device)\n",
    "model.prune(sparsity, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c9f5c7",
   "metadata": {},
   "source": [
    "### Running the training\n",
    "\n",
    "Below, we run the quantization-aware training, which can be quite slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afd06825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing MNIST task with 2-bits in quantization and a sparsity of 4\n",
      "\n",
      "1. Training\n",
      "Train Epoch: 1 [0/1875 (0%)]     \tLoss: 6.618234\n",
      "Train Epoch: 1 [500/1875 (27%)]     \tLoss: 0.823759\n",
      "Train Epoch: 1 [1000/1875 (53%)]     \tLoss: 0.708366\n",
      "Train Epoch: 1 [1500/1875 (80%)]     \tLoss: 0.393239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:02<00:00, 122.09it/s]\n",
      "/home/dev_user/dev_venv/lib/python3.8/site-packages/brevitas/quant_tensor/__init__.py:65: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  training = torch.tensor(training, dtype=torch.bool)\n",
      "/home/dev_user/dev_venv/lib/python3.8/site-packages/brevitas/quant_tensor/__init__.py:63: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  signed = torch.tensor(signed, dtype=torch.bool)\n",
      "/home/dev_user/dev_venv/lib/python3.8/site-packages/brevitas/quant_tensor/__init__.py:71: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  return self.signed_t.item()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0191, Accuracy: 8222/10000 (82%)\n",
      "\n",
      "2. Exporting to ONNX and saving the Brevitas model\n"
     ]
    }
   ],
   "source": [
    "# Training part\n",
    "print(\n",
    "    f\"Performing MNIST task with {quantization_bits}-bits in quantization and a \"\n",
    "    f\"sparsity of {sparsity}\"\n",
    ")\n",
    "\n",
    "if do_training:\n",
    "    print(\"\\n1. Training\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0.0001)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "    loss_values = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train(model, device, train_loader, optimizer, epoch, criterion)\n",
    "        cur_loss = test(model, device, test_loader, epoch, criterion)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        loss_values.append(cur_loss)\n",
    "\n",
    "    model.prune(sparsity, False)\n",
    "\n",
    "    # Plot the loss\n",
    "    fig = plt.figure()\n",
    "    plt.plot(loss_values)\n",
    "    fig.suptitle(\"Loss during QAT\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xticks(range(len(loss_values)))\n",
    "\n",
    "    # Export to ONNX\n",
    "    print(\"\\n2. Exporting to ONNX and saving the Brevitas model\")\n",
    "    inp = torch.rand((1, img_size * img_size)).to(device)\n",
    "    torch.onnx.export(model, inp, \"mnist.qat.onnx\", opset_version=14)\n",
    "    torch.save(model.state_dict(), \"state_dict.pt\")\n",
    "else:\n",
    "    print(\"\\n1. Loading pre-trained model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e909ba2",
   "metadata": {},
   "source": [
    "To avoid the training phase, you can directly load the pre-trained model by setting `do_training = False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93eca8f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"state_dict.pt\")\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce19d042",
   "metadata": {},
   "source": [
    "We prepare the ground truth, for final check of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3736e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:01<00:00, 294.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare tests\n",
    "test_data = np.zeros((len(test_loader.dataset), img_size * img_size))\n",
    "test_target = np.zeros((len(test_loader.dataset), 1))\n",
    "idx = 0\n",
    "\n",
    "for data, target in tqdm(test_loader):\n",
    "    target_np = target.cpu().numpy()\n",
    "    for idx_batch, im in enumerate(data.numpy()):\n",
    "        test_data[idx] = im\n",
    "        test_target[idx] = target_np[idx_batch]\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14025865",
   "metadata": {},
   "source": [
    "### Measuring accuracy in VL and FHE\n",
    "\n",
    "Finally, we measure:\n",
    "- accuracy in VL for the full test set\n",
    "- accuracy in VL for the reduced test set\n",
    "- accuracy in FHE for the reduced test set   \n",
    "\n",
    "and we check that accuracy in VL and in FHE are equivalent on the reduced test set. \n",
    "\n",
    "Running FHE computations on a too large reduced test set may be prohibitive since we're 100% on CPU for the time being. Later, we'll have HW accelerators, to make these computations several order of magnitude faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb014141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Compiling with the Virtual Library\n",
      "\n",
      "4. Checking accuracy with the Virtual Library (length 10000)\n",
      "Accuracy in VL with length 10000: 8222/10000 = 0.8222, in 6-bits\n",
      "\n",
      "5. Compiling with the Virtual Library\n",
      "\n",
      "6. Checking accuracy with the Virtual Library (length 2)\n",
      "Accuracy in VL with length 2: 2/2 = 1.0000, in 6-bits\n",
      "\n",
      "7. Compiling in FHE\n",
      "\n",
      "8. Checking accuracy in FHE (length 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KeySetCache: miss, regenerating /tmp/keycache/1739235842309079810/0_0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in FHE with length 2: 2/2 = 1.0000, in 6-bits\n"
     ]
    }
   ],
   "source": [
    "# Test in the VL and in FHE\n",
    "accuracy = {}\n",
    "current_index = 3\n",
    "\n",
    "for use_virtual_lib, use_full_dataset in [(True, True), (True, False), (False, False)]:\n",
    "    test_data_length = test_data_length_full if use_full_dataset else test_data_length_reduced\n",
    "\n",
    "    correct_fhe, test_data_shape_0, max_bit_width = compile_and_test(\n",
    "        model.cpu(),\n",
    "        use_virtual_lib,\n",
    "        test_data,\n",
    "        test_data_length,\n",
    "        test_target,\n",
    "        show_mlir,\n",
    "        current_index,\n",
    "    )\n",
    "\n",
    "    current_index += 2\n",
    "    current_accuracy = correct_fhe / test_data_shape_0\n",
    "\n",
    "    print(\n",
    "        f\"Accuracy in {'VL' if use_virtual_lib else 'FHE'} with length {test_data_length}: \"\n",
    "        f\"{correct_fhe}/{test_data_shape_0} = \"\n",
    "        f\"{current_accuracy:.4f}, in {max_bit_width}-bits\"\n",
    "    )\n",
    "\n",
    "    if (use_virtual_lib, use_full_dataset) == (True, True):\n",
    "        accuracy[\"VL full\"] = current_accuracy\n",
    "    elif (use_virtual_lib, use_full_dataset) == (True, False):\n",
    "        accuracy[\"VL short\"] = current_accuracy\n",
    "    else:\n",
    "        assert (use_virtual_lib, use_full_dataset) == (False, False)\n",
    "        accuracy[\"FHE short\"] = current_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf39f067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that accuracy in FHE and in VL is the same\n",
    "assert (\n",
    "    accuracy[\"VL short\"] == accuracy[\"FHE short\"]\n",
    "), \"Error, accuracy in VL and in FHE are not the same\"\n",
    "\n",
    "# Check that accuracy is random-looking\n",
    "assert accuracy[\"VL full\"] > 0.8, \"Error, accuracy is too bad\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5060f56",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "We've shown how to train a model with Brevitas in quantization-aware training and how easy it is to obtain a satisfactory accuray of 92% even if it is lower than the state-of-the-art on cleartexts. \n",
    "\n",
    "Brevitas is not the only third-party package one can use for QAT. Other examples with other frameworks will certainly be added in our repository."
   ]
  }
 ],
 "metadata": {
  "execution": {
   "timeout": 10800
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
