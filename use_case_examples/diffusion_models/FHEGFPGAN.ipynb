{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/celia/Desktop/Zama/concrete-internal/.dm/lib/python3.8/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "from copy import copy\n",
    "from pprint import pprint\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "from basicsr.utils import imwrite\n",
    "from gfpgan import GFPGANer\n",
    "from gfpgan.archs.stylegan2_clean_arch import ModulatedConv2d\n",
    "from realesrgan import RealESRGANer\n",
    "from tqdm import tqdm\n",
    "from utils import *\n",
    "\n",
    "from concrete.ml.torch.hybrid_model import HybridFHEModel\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.input = \"GFPGAN/inputs/whole_imgs\"\n",
    "        self.output = \"results\"\n",
    "        self.version = \"1.4\"\n",
    "        self.upscale = 5\n",
    "        self.bg_upsampler = \"realesrgan\"\n",
    "        self.bg_tile = 400\n",
    "        self.suffix = None\n",
    "        self.only_center_face = False\n",
    "        self.aligned = False\n",
    "        self.ext = \"auto\"\n",
    "        self.weight = 0.5\n",
    "\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = sorted(glob.glob(f\"{args.input}/*\"))\n",
    "\n",
    "os.makedirs(args.output, exist_ok=True)\n",
    "\n",
    "assert len(img_list) >= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_background_improvement = True\n",
    "\n",
    "if args.bg_upsampler == \"realesrgan\":\n",
    "    if use_background_improvement:\n",
    "\n",
    "        half = True if torch.cuda.is_available() else False\n",
    "\n",
    "        model = RRDBNet(\n",
    "            num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=2\n",
    "        )\n",
    "        # No linear modules in this model\n",
    "        bg_upsampler = RealESRGANer(\n",
    "            scale=2,  # Do not change this value\n",
    "            model_path=\"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth\",\n",
    "            model=model,\n",
    "            tile=args.bg_tile,\n",
    "            tile_pad=10,\n",
    "            pre_pad=0,\n",
    "            half=half,\n",
    "        )  # need to set False in CPU mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.version == \"1.3\":\n",
    "    arch = \"clean\"\n",
    "    channel_multiplier = 2\n",
    "    model_name = \"GFPGANv1.3\"\n",
    "    url = \"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth\"\n",
    "    local_model_path = \"GFPGANv1.3.pth\"\n",
    "elif args.version == \"1.4\":\n",
    "    arch = \"clean\"\n",
    "    channel_multiplier = 2\n",
    "    model_name = \"GFPGANv1.4\"\n",
    "    url = \"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth\"\n",
    "    local_model_path = \"GFPGANv1.4.pth\"\n",
    "\n",
    "# determine model paths\n",
    "model_path = os.path.join(\"experiments/pretrained_models\", model_name + \".pth\")\n",
    "if not os.path.isfile(model_path):\n",
    "    model_path = os.path.join(\"gfpgan/weights\", model_name + \".pth\")\n",
    "if not os.path.isfile(model_path):\n",
    "    # download pre-trained models from url\n",
    "    model_path = url\n",
    "\n",
    "restorer = GFPGANer(\n",
    "    model_path=model_path,\n",
    "    upscale=args.upscale,\n",
    "    arch=arch,\n",
    "    channel_multiplier=channel_multiplier,\n",
    "    bg_upsampler=bg_upsampler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Face cropping and extraction\n",
    "# using a FaceRestoreHelper with retinaface_resnet50\n",
    "# No linear layers\n",
    "\n",
    "face_helper_model = restorer.face_helper.face_det\n",
    "face_helper_state_dict = restorer.face_helper.face_det.state_dict()\n",
    "\n",
    "gfpgan = copy(restorer.gfpgan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 79 23\n",
      "24 79 23\n"
     ]
    }
   ],
   "source": [
    "gfpgan_linear = extract_specific_module(gfpgan, dtype_layer=torch.nn.Linear, verbose=False)\n",
    "\n",
    "gfpgan_conv2d = extract_specific_module(gfpgan, dtype_layer=torch.nn.Conv2d, verbose=False)\n",
    "\n",
    "gfpgan_modulated_conv2d = extract_specific_module(gfpgan, dtype_layer=ModulatedConv2d, verbose=False)\n",
    "\n",
    "print(len(gfpgan_linear), len(gfpgan_conv2d), len(gfpgan_modulated_conv2d))\n",
    "\n",
    "gfpgan_linear = [\n",
    "        name\n",
    "        for name, _ in gfpgan_linear\n",
    "        if restorer.gfpgan.input_is_latent and \"style_mlp\" not in name\n",
    "    ]\n",
    "\n",
    "print(len(gfpgan_linear), len(gfpgan_conv2d), len(gfpgan_modulated_conv2d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_size = (3, 512, 512)\n",
    "\n",
    "compile_size = 2\n",
    "inputs = torch.randn((compile_size, *input_size))\n",
    "\n",
    "hybrid_model = HybridFHEModel(\n",
    "    gfpgan,\n",
    "    gfpgan_linear,\n",
    "    verbose=2,\n",
    ")\n",
    "# Compile hybrid model\n",
    "hybrid_model.compile_model(\n",
    "    inputs,\n",
    "    n_bits=8,\n",
    ")\n",
    "\n",
    "# summary(restorer.gfpgan, input_size=input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = hybrid_model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (3, 256, 256)\n",
    "inputs = torch.randn((compile_size, *input_size))\n",
    "_ = hybrid_model(torch.randn((1, 3, 512, 512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_path in tqdm(img_list):\n",
    "    # read image\n",
    "    img_name = os.path.basename(img_path)\n",
    "    print(f\"Processing {img_name} ...\")\n",
    "    basename, ext = os.path.splitext(img_name)\n",
    "    input_img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "    print(f\"{input_img.shape=}\")\n",
    "\n",
    "    # restore faces and background if necessary\n",
    "    cropped_faces, restored_faces, restored_img = restorer.enhance(\n",
    "        input_img,\n",
    "        has_aligned=args.aligned,\n",
    "        only_center_face=args.only_center_face,\n",
    "        paste_back=True,\n",
    "        weight=args.weight,\n",
    "    )\n",
    "\n",
    "    # print(f\"{len(cropped_faces)=} | {len(restored_faces)=} | {restored_img.shape=}\")\n",
    "\n",
    "    # save faces\n",
    "    for idx, (cropped_face, restored_face) in tqdm(enumerate(zip(cropped_faces, restored_faces))):\n",
    "        # save cropped face\n",
    "        save_crop_path = os.path.join(args.output, \"cropped_faces\", f\"{basename}_{idx:02d}.png\")\n",
    "        imwrite(cropped_face, save_crop_path)\n",
    "        # save restored face\n",
    "        if args.suffix is not None:\n",
    "            save_face_name = f\"{basename}_{idx:02d}_{args.suffix}.png\"\n",
    "        else:\n",
    "            save_face_name = f\"{basename}_{idx:02d}.png\"\n",
    "        save_restore_path = os.path.join(args.output, \"restored_faces\", save_face_name)\n",
    "        imwrite(restored_face, save_restore_path)\n",
    "        # save comparison image\n",
    "        cmp_img = np.concatenate((cropped_face, restored_face), axis=1)\n",
    "        imwrite(cmp_img, os.path.join(args.output, \"cmp\", f\"{basename}_{idx:02d}.png\"))\n",
    "\n",
    "    # save restored img\n",
    "    if restored_img is not None:\n",
    "        if args.ext == \"auto\":\n",
    "            extension = ext[1:]\n",
    "        else:\n",
    "            extension = args.ext\n",
    "\n",
    "        if args.suffix is not None:\n",
    "            save_restore_path = os.path.join(\n",
    "                args.output, \"restored_imgs\", f\"{basename}_{args.suffix}.{extension}\"\n",
    "            )\n",
    "        else:\n",
    "            save_restore_path = os.path.join(\n",
    "                args.output, \"restored_imgs\", f\"{basename}.{extension}\"\n",
    "            )\n",
    "        imwrite(restored_img, save_restore_path)\n",
    "        break\n",
    "\n",
    "print(f\"Results are in the [{args.output}] folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "execution": {
   "timeout": 10800
  },
  "kernelspec": {
   "display_name": ".dm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
