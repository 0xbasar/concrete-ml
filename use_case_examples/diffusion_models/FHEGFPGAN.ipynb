{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/celia/Desktop/Zama/concrete-internal/.dm/lib/python3.8/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "from copy import copy\n",
    "from pprint import pprint\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "from basicsr.utils import imwrite\n",
    "from gfpgan import GFPGANer\n",
    "from gfpgan.archs.stylegan2_clean_arch import ModulatedConv2d\n",
    "from realesrgan import RealESRGANer\n",
    "from tqdm import tqdm\n",
    "from utils import *\n",
    "\n",
    "from concrete.ml.torch.hybrid_model import HybridFHEModel\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of the GFP-GAN Architecture\n",
    "\n",
    "The GFP-GAN pipeline is divided into 3 main components:\n",
    "\n",
    "1. Face Cropping (restorer.face_helper): To detect and crop faces from input images.\n",
    "\n",
    "    Composition:\n",
    "    - 82 Standard Convolutional Layers with:\n",
    "        + Kernel Sizes: (7, 7) or (3, 3) or (1, 1)\n",
    "        + Strides: (2, 2) or (1, 1)\n",
    "        + Padding: (3, 3) or (1, 1)\n",
    "\n",
    "\n",
    "    - **Absence of Grouped or Dilated Convolutions or Depthwise Convolutions**\n",
    "\n",
    "        + Grouped Convolutions: Convolutions where the input is divided into parts/groups, we have a set of filters for each group, the result is concatenated.\n",
    "        (groups=1 by default).\n",
    "\n",
    "        + Dilated Convolutions: Convolutions where the kernel is expanded by inserting zeros between its elements, increasing the receptive field without increasing the number of parameters.\n",
    "        (dilation=1 by default).\n",
    "\n",
    "    - **Modulated**:\n",
    "\n",
    "        + The convolutional weights are dynamically adjusted (modulated) for each input sample based on a style vector.\n",
    "        + This modulation allows the network to adapt its convolutional filters per sample, enabling more control over generated features.\n",
    "        + Modulate Weights Process: For each ModulatedConv2d layer:\n",
    "            - The style vector is transformed (usually via another linear layer) to obtain modulation weights.\n",
    "            - These weights modulate the convolutional filters.\n",
    "            - Demodulation: After modulation, weights can vary in magnitude, leading to instability during training. Demodulation normalizes the weights to maintain a consistent signal magnitude across the layers.\n",
    "\n",
    "\n",
    "        ```python\n",
    "        Style Vector (w)\n",
    "                |\n",
    "        Modulation Weights (s)\n",
    "                |\n",
    "        Modulated Weights (s * k)\n",
    "                |\n",
    "        (Optional) Demodulation\n",
    "                |\n",
    "        Convolution Operation\n",
    "                |\n",
    "        Output Feature Maps\n",
    "        ```\n",
    "\n",
    "2. Face Restoration (restorer.gfpgan): To restore and enhance the quality of cropped facial images.\n",
    "\n",
    "    Composition:\n",
    "    - 32 Linear Layers\n",
    "    - 79 Standard Convolutional Layers with:\n",
    "        + Kernel Sizes: (3, 3) or (1, 1)\n",
    "        + Strides: (1, 1)\n",
    "        + Padding: (1, 1)\n",
    "    - 23 Modulated Convolutional Layers (ModulatedConv2d), with: Kernel Sizes: 3 or 11\n",
    "\n",
    "3. Background Enhancement (restorer.upsampler): To enhance the background details of the images after face restoration.\n",
    "\n",
    "    Composition:\n",
    "    - 351 Standard Convolutional Layers with fixed configurations:\n",
    "        + Kernel Size: (3, 3)\n",
    "        + Stride: (1, 1)\n",
    "        + Padding: (1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.input = \"GFPGAN/inputs/whole_imgs\"\n",
    "        self.output = \"results\"\n",
    "        self.version = \"1.4\"\n",
    "        self.upscale = 5\n",
    "        self.bg_upsampler = \"realesrgan\"\n",
    "        self.bg_tile = 400\n",
    "        self.suffix = None\n",
    "        self.only_center_face = False\n",
    "        self.aligned = False\n",
    "        self.ext = \"auto\"\n",
    "        self.weight = 0.5\n",
    "\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = sorted(glob.glob(f\"{args.input}/*\"))\n",
    "\n",
    "os.makedirs(args.output, exist_ok=True)\n",
    "\n",
    "assert len(img_list) >= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_background_improvement = True\n",
    "\n",
    "if args.bg_upsampler == \"realesrgan\":\n",
    "    if use_background_improvement:\n",
    "\n",
    "        half = True if torch.cuda.is_available() else False\n",
    "\n",
    "        model = RRDBNet(\n",
    "            num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=2\n",
    "        )\n",
    "        # No linear modules in this model\n",
    "        bg_upsampler = RealESRGANer(\n",
    "            scale=2,  # Do not change this value\n",
    "            model_path=\"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth\",\n",
    "            model=model,\n",
    "            tile=args.bg_tile,\n",
    "            tile_pad=10,\n",
    "            pre_pad=0,\n",
    "            half=half,\n",
    "        )  # need to set False in CPU mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-----------------------: model_path='https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth'- args.version='1.4'\n",
      "\n",
      "-----------------------: model_path='https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth'- args.version='1.4'\n",
      "\n",
      "-----------------------: restorer=<gfpgan.utils.GFPGANer object at 0x71a565f1d520>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if args.version == \"1\":\n",
    "    arch = \"original\"\n",
    "    channel_multiplier = 1\n",
    "    model_name = \"GFPGANv1\"\n",
    "    url = \"https://github.com/TencentARC/GFPGAN/releases/download/v0.1.0/GFPGANv1.pth\"\n",
    "elif args.version == \"1.2\":\n",
    "    arch = \"clean\"\n",
    "    channel_multiplier = 2\n",
    "    model_name = \"GFPGANCleanv1-NoCE-C2\"\n",
    "    url = \"https://github.com/TencentARC/GFPGAN/releases/download/v0.2.0/GFPGANCleanv1-NoCE-C2.pth\"\n",
    "elif args.version == \"1.3\":\n",
    "    arch = \"clean\"\n",
    "    channel_multiplier = 2\n",
    "    model_name = \"GFPGANv1.3\"\n",
    "    url = \"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth\"\n",
    "    local_model_path = \"GFPGANv1.3.pth\"\n",
    "elif args.version == \"1.4\":\n",
    "    arch = \"clean\"\n",
    "    channel_multiplier = 2\n",
    "    model_name = \"GFPGANv1.4\"\n",
    "    url = \"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth\"\n",
    "    local_model_path = \"GFPGANv1.4.pth\"\n",
    "elif args.version == \"RestoreFormer\":\n",
    "    arch = \"RestoreFormer\"\n",
    "    channel_multiplier = 2\n",
    "    model_name = \"RestoreFormer\"\n",
    "    url = \"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.4/RestoreFormer.pth\"\n",
    "else:\n",
    "    raise ValueError(f\"Wrong model version {args.version}.\")\n",
    "\n",
    "# determine model paths\n",
    "model_path = os.path.join(\"experiments/pretrained_models\", model_name + \".pth\")\n",
    "if not os.path.isfile(model_path):\n",
    "    model_path = os.path.join(\"gfpgan/weights\", model_name + \".pth\")\n",
    "if not os.path.isfile(model_path):\n",
    "    # download pre-trained models from url\n",
    "    model_path = url\n",
    "\n",
    "print(f\"\\n\\n-----------------------: {model_path=}- {args.version=}\\n\")\n",
    "\n",
    "restorer = GFPGANer(\n",
    "    model_path=model_path,\n",
    "    upscale=args.upscale,\n",
    "    arch=arch,\n",
    "    channel_multiplier=channel_multiplier,\n",
    "    bg_upsampler=bg_upsampler,\n",
    ")\n",
    "\n",
    "print(f\"-----------------------: {model_path=}- {args.version=}\\n\")\n",
    "\n",
    "print(f\"-----------------------: {restorer=}\\n\")\n",
    "\n",
    "assert isinstance(restorer.gfpgan, torch.nn.Module) is True\n",
    "assert isinstance(model, torch.nn.Module) is True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Face cropping and extraction\n",
    "# using a FaceRestoreHelper with retinaface_resnet50\n",
    "# No linear layers\n",
    "\n",
    "face_helper_model = restorer.face_helper.face_det\n",
    "face_helper_state_dict = restorer.face_helper.face_det.state_dict()\n",
    "\n",
    "restorer.gfpgan =restorer.gfpgan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 79 23\n"
     ]
    }
   ],
   "source": [
    "restorer_gfpgan_linear = extract_specific_module(\n",
    "    restorer.gfpgan, dtype_layer=torch.nn.Linear, verbose=False\n",
    ")\n",
    "\n",
    "restorer_gfpgan_conv2d = extract_specific_module(\n",
    "    restorer.gfpgan, dtype_layer=torch.nn.Conv2d, verbose=False\n",
    ")\n",
    "\n",
    "restorer_gfpgan_modulated_conv2d = extract_specific_module(\n",
    "    restorer.gfpgan, dtype_layer=ModulatedConv2d, verbose=False\n",
    ")\n",
    "\n",
    "\n",
    "print(\n",
    "    len(restorer_gfpgan_linear), len(restorer_gfpgan_conv2d), len(restorer_gfpgan_modulated_conv2d)\n",
    ")\n",
    "\n",
    "model = copy(restorer.gfpgan)\n",
    "\n",
    "input_size = (3, 512, 512)\n",
    "\n",
    "compile_size = 2\n",
    "inputs = torch.randn((compile_size, *input_size))\n",
    "\n",
    "hybrid_model = HybridFHEModel(\n",
    "    model,\n",
    "    [\n",
    "        name\n",
    "        for name, _ in restorer_gfpgan_linear\n",
    "        if restorer.gfpgan.input_is_latent and \"style_mlp\" not in name\n",
    "    ],\n",
    "    verbose=2,\n",
    ")\n",
    "# Compile hybrid model\n",
    "hybrid_model.compile_model(\n",
    "    inputs,\n",
    "    n_bits=8,\n",
    ")\n",
    "\n",
    "# summary(restorer.gfpgan, input_size=input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = hybrid_model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (3, 256, 256)\n",
    "inputs = torch.randn((compile_size, *input_size))\n",
    "_ = hybrid_model(torch.randn((1, 3, 512, 512)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 351\n"
     ]
    }
   ],
   "source": [
    "isinstance(restorer.bg_upsampler.model, torch.nn.Module)\n",
    "\n",
    "restorer_upsampler_linear = extract_specific_module(\n",
    "    restorer.bg_upsampler.model, dtype_layer=torch.nn.Linear, verbose=False\n",
    ")\n",
    "\n",
    "restorer_upsampler_conv2d = extract_specific_module(\n",
    "    restorer.bg_upsampler.model, dtype_layer=torch.nn.Conv2d, verbose=False\n",
    ")\n",
    "\n",
    "print(len(restorer_upsampler_linear), len(restorer_upsampler_conv2d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 00.jpg ...\n",
      "input_img.shape=(1920, 1297, 3)\n",
      "(1920, 1297, 3) iciii\n",
      "a la fin (1920, 1297, 3)\n",
      "MAINTENAT:  (1920, 1297, 3)\n",
      "detect faces (1920, 1297, 3)\n",
      "Inside detect faces: image.shape=torch.Size([1, 3, 1920, 1297])\n",
      "self.half_inference=False, image.shape=torch.Size([1, 3, 1920, 1297])\n",
      "type(bboxes)=<class 'numpy.ndarray'>, bboxes.shape=(2, 15)\n",
      "2 khksdgkjhdbgkjhsbq\n",
      "laaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa torch.Size([1, 3, 512, 512])\n",
      "laaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa torch.Size([1, 3, 512, 512])\n",
      "\tTile 1/20\n",
      "\tTile 2/20\n",
      "\tTile 3/20\n",
      "\tTile 4/20\n",
      "\tTile 5/20\n",
      "\tTile 6/20\n",
      "\tTile 7/20\n",
      "\tTile 8/20\n",
      "\tTile 9/20\n",
      "\tTile 10/20\n"
     ]
    }
   ],
   "source": [
    "for img_path in tqdm(img_list):\n",
    "    # read image\n",
    "    img_name = os.path.basename(img_path)\n",
    "    print(f\"Processing {img_name} ...\")\n",
    "    basename, ext = os.path.splitext(img_name)\n",
    "    input_img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "    print(f\"{input_img.shape=}\")\n",
    "\n",
    "    # restore faces and background if necessary\n",
    "    cropped_faces, restored_faces, restored_img = restorer.enhance(\n",
    "        input_img,\n",
    "        has_aligned=args.aligned,\n",
    "        only_center_face=args.only_center_face,\n",
    "        paste_back=True,\n",
    "        weight=args.weight,\n",
    "    )\n",
    "\n",
    "    # print(f\"{len(cropped_faces)=} | {len(restored_faces)=} | {restored_img.shape=}\")\n",
    "\n",
    "    # save faces\n",
    "    for idx, (cropped_face, restored_face) in tqdm(enumerate(zip(cropped_faces, restored_faces))):\n",
    "        # save cropped face\n",
    "        save_crop_path = os.path.join(args.output, \"cropped_faces\", f\"{basename}_{idx:02d}.png\")\n",
    "        imwrite(cropped_face, save_crop_path)\n",
    "        # save restored face\n",
    "        if args.suffix is not None:\n",
    "            save_face_name = f\"{basename}_{idx:02d}_{args.suffix}.png\"\n",
    "        else:\n",
    "            save_face_name = f\"{basename}_{idx:02d}.png\"\n",
    "        save_restore_path = os.path.join(args.output, \"restored_faces\", save_face_name)\n",
    "        imwrite(restored_face, save_restore_path)\n",
    "        # save comparison image\n",
    "        cmp_img = np.concatenate((cropped_face, restored_face), axis=1)\n",
    "        imwrite(cmp_img, os.path.join(args.output, \"cmp\", f\"{basename}_{idx:02d}.png\"))\n",
    "\n",
    "    # save restored img\n",
    "    if restored_img is not None:\n",
    "        if args.ext == \"auto\":\n",
    "            extension = ext[1:]\n",
    "        else:\n",
    "            extension = args.ext\n",
    "\n",
    "        if args.suffix is not None:\n",
    "            save_restore_path = os.path.join(\n",
    "                args.output, \"restored_imgs\", f\"{basename}_{args.suffix}.{extension}\"\n",
    "            )\n",
    "        else:\n",
    "            save_restore_path = os.path.join(\n",
    "                args.output, \"restored_imgs\", f\"{basename}.{extension}\"\n",
    "            )\n",
    "        imwrite(restored_img, save_restore_path)\n",
    "        break\n",
    "\n",
    "print(f\"Results are in the [{args.output}] folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "execution": {
   "timeout": 10800
  },
  "kernelspec": {
   "display_name": ".dm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
