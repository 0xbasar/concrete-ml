{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/celia/Desktop/Zama/concrete-internal/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "from copy import copy\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import concrete.fhe\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "from basicsr.utils import img2tensor, imwrite, tensor2img\n",
    "from gfpgan.archs.stylegan2_clean_arch import ModulatedConv2d\n",
    "from gfpgan_utils import *\n",
    "from realesrgan import RealESRGANer\n",
    "from torchvision.transforms.functional import normalize\n",
    "from tqdm import tqdm\n",
    "\n",
    "from concrete.ml.torch.hybrid_model import HybridFHEModel\n",
    "from gfpgan import GFPGANer\n",
    "\n",
    "# !pip install concrete-ml\n",
    "# !pip install basicsr\n",
    "# !pip install facexlib\n",
    "# !pip install realesrgan\n",
    "# !pip install lmdb\n",
    "# !pip install opencv-python\n",
    "# !pip install pyyaml\n",
    "# !pip install tb-nightly\n",
    "# !pip install yapf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.input = \"GFPGAN/inputs/whole_imgs\"\n",
    "        self.output = Path(\"results\")\n",
    "        self.version = \"1.4\"\n",
    "        self.upscale = 5\n",
    "        self.bg_upsampler = \"realesrgan\"\n",
    "        self.bg_tile = 400\n",
    "        self.suffix = None\n",
    "        self.only_center_face = False\n",
    "        self.aligned = False\n",
    "        self.ext = \"auto\"\n",
    "        self.weight = 0.5\n",
    "\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GFPGAN/inputs/whole_imgs/00.jpg']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_list = sorted(glob.glob(f\"{args.input}/*\"))[:1]\n",
    "\n",
    "os.makedirs(args.output, exist_ok=True)\n",
    "\n",
    "assert len(img_list) >= 1\n",
    "\n",
    "img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GFPGANFHEHybridWrapper(GFPGANer):\n",
    "    def __init__(self, **kwargs):\n",
    "\n",
    "        super().__init__(\n",
    "            model_path=get_gfpgan_path(),\n",
    "            upscale=kwargs.get(\"upscale\", 2),\n",
    "            arch=\"clean\",\n",
    "            channel_multiplier=2,\n",
    "            bg_upsampler=RealESRGANer(\n",
    "                scale=2,\n",
    "                model_path=\"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth\",\n",
    "                model=RRDBNet(\n",
    "                    num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=2\n",
    "                ),\n",
    "                tile=kwargs.get(\"bg_tile\", 0),\n",
    "                tile_pad=10,\n",
    "                pre_pad=0,\n",
    "                half=torch.cuda.is_available(),\n",
    "            ),\n",
    "            device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        )\n",
    "\n",
    "        # Model 1\n",
    "        self.face_helper = self.face_helper\n",
    "\n",
    "        # Model 2\n",
    "        self.base_gfpgan = self.gfpgan\n",
    "        self.hybrid_gfpgan = None\n",
    "\n",
    "        # Model3\n",
    "        self.upsampler = self.bg_upsampler.model\n",
    "\n",
    "    def _prepare_faces(self, img, has_aligned, only_center_face):\n",
    "        \"\"\"Prepare faces by either resizing (aligned) or detecting landmarks (unaligned).\"\"\"\n",
    "        if has_aligned:\n",
    "            img = cv2.resize(img, (512, 512))\n",
    "            self.face_helper.cropped_faces = [img]\n",
    "        else:\n",
    "            self.face_helper.read_image(img)\n",
    "            # Get face landmarks\n",
    "            self.face_helper.get_face_landmarks_5(\n",
    "                only_center_face=only_center_face, eye_dist_threshold=5\n",
    "            )\n",
    "            # Align and warp each face\n",
    "            self.face_helper.align_warp_face()\n",
    "\n",
    "    def _prepare_data(self, cropped_face):\n",
    "        # Prepare data\n",
    "        cropped_face_t = img2tensor(cropped_face / 255.0, bgr2rgb=True, float32=True)\n",
    "        normalize(cropped_face_t, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True)\n",
    "        cropped_face_t = cropped_face_t.unsqueeze(0).to(self.device)\n",
    "        return cropped_face_t\n",
    "\n",
    "    def _finalyze(self, img, has_aligned, paste_back):\n",
    "\n",
    "        if not has_aligned and paste_back:\n",
    "            bg_img = None\n",
    "            if self.bg_upsampler is not None:\n",
    "                bg_img = self.bg_upsampler.enhance(img, outscale=self.upscale)[0]\n",
    "\n",
    "            self.face_helper.get_inverse_affine(None)\n",
    "            restored_img = self.face_helper.paste_faces_to_input_image(upsample_img=bg_img)\n",
    "            return self.face_helper.cropped_faces, self.face_helper.restored_faces, restored_img\n",
    "        else:\n",
    "            return self.face_helper.cropped_faces, self.face_helper.restored_faces, None\n",
    "\n",
    "    def forward(self, img, fhe_mode=\"disable\", **kwargs):\n",
    "        \"\"\"Forward pass with FHE mode, handling face preparation and restoration.\"\"\"\n",
    "\n",
    "        assert self.hybrid_gfpgan is not None, \"Hybrid model is not initialized. Please run `use_hybrid_model` before.\"\n",
    "\n",
    "        has_aligned = kwargs.get(\"has_aligned\", False)\n",
    "        only_center_face = kwargs.get(\"only_center_face\", False)\n",
    "        paste_back = kwargs.get(\"paste_back\", True)\n",
    "        weight = kwargs.get(\"weight\", 0.5)\n",
    "        return_rgb = kwargs.get(\"return_rgb\", False)\n",
    "\n",
    "        self.face_helper.clean_all()\n",
    "\n",
    "        # Prepare aligned or unaligned faces\n",
    "        self._prepare_faces(img, has_aligned, only_center_face)\n",
    "\n",
    "        # Face restoration\n",
    "        for cropped_face in self.face_helper.cropped_faces:\n",
    "\n",
    "            cropped_face_t = self._prepare_data(cropped_face)\n",
    "\n",
    "            output = self.hybrid_gfpgan(\n",
    "                cropped_face_t, fhe_mode=fhe_mode, **{\"return_rgb\": return_rgb, \"weight\": weight}\n",
    "            )[0]\n",
    "\n",
    "            # Convert to image\n",
    "            restored_face = tensor2img(output.squeeze(0), rgb2bgr=True, min_max=(-1, 1)).astype(\n",
    "                \"uint8\"\n",
    "            )\n",
    "\n",
    "            self.face_helper.add_restored_face(restored_face)\n",
    "\n",
    "        # Upsample and paste back faces to the original image\n",
    "        return self._finalyze(img, has_aligned, paste_back)\n",
    "    \n",
    "    def __call__(self, img, fhe_mode, **kwargs):\n",
    "        return self.forward(img, fhe_mode, **kwargs)\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        inputs,\n",
    "        n_bits=5,\n",
    "    ):\n",
    "        \"\"\"Compile the hybrid model.\"\"\"\n",
    "\n",
    "        if not self.hybrid_gfpgan:\n",
    "            raise ValueError(\n",
    "                \"Hybrid model is not initialized. Please run `use_hybrid_model` before compiling.\"\n",
    "            )\n",
    "\n",
    "        # Compile hybrid model\n",
    "        self.hybrid_gfpgan.compile_model(\n",
    "            inputs,\n",
    "            n_bits=n_bits,\n",
    "        )\n",
    "\n",
    "    def use_hybrid_model(self, remote_layers):\n",
    "        \"\"\"Initialize the hybrid model with specified remote layers.\"\"\"\n",
    "        if remote_layers == []:\n",
    "            print(\"No remote layers specified; using the base model.\")\n",
    "        self.hybrid_gfpgan = HybridFHEModel(self.base_gfpgan, remote_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_gfpgan = GFPGANFHEHybridWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(linear_layer_names)=24 - len(conv_layer_names)=79\n"
     ]
    }
   ],
   "source": [
    "linear_layer_names = extract_specific_module(\n",
    "    wrapped_gfpgan.base_gfpgan, dtype_layer=torch.nn.Linear\n",
    ")\n",
    "\n",
    "linear_layer_names = [\n",
    "    name\n",
    "    for name in linear_layer_names\n",
    "    if wrapped_gfpgan.base_gfpgan.input_is_latent and \"style_mlp\" not in name\n",
    "]\n",
    "\n",
    "conv_layer_names = extract_specific_module(wrapped_gfpgan.base_gfpgan, dtype_layer=torch.nn.Conv2d)\n",
    "\n",
    "print(f\"{len(linear_layer_names)=} - {len(conv_layer_names)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_gfpgan.use_hybrid_model(linear_layer_names)\n",
    "\n",
    "input_size = (3, 512, 512)\n",
    "inputs = torch.randn((1, *input_size))\n",
    "\n",
    "wrapped_gfpgan.compile(inputs, n_bits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid model with fhe_mode='execute'\n"
     ]
    }
   ],
   "source": [
    "fhe_mode = \"execute\"\n",
    "extra = \"conv_&_linear\"\n",
    "\n",
    "for img_path in tqdm(img_list):\n",
    "\n",
    "    input_img, basename, ext = read_img(img_path)\n",
    "\n",
    "    cropped_faces, restored_faces, restored_img = wrapped_gfpgan(input_img, fhe_mode=fhe_mode)\n",
    "\n",
    "    save_restored_faces(\n",
    "        cropped_faces,\n",
    "        restored_faces,\n",
    "        restored_img,\n",
    "        Path(args.output) / f\"{extra}_{fhe_mode}\",\n",
    "        basename,\n",
    "        args.suffix,\n",
    "        ext,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "execution": {
   "timeout": 10800
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
