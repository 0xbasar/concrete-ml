{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/celia/Desktop/Zama/concrete-internal/.dm/lib/python3.8/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "from copy import copy\n",
    "from pprint import pprint\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "from basicsr.utils import imwrite\n",
    "from gfpgan import GFPGANer\n",
    "from gfpgan.archs.stylegan2_clean_arch import ModulatedConv2d\n",
    "from realesrgan import RealESRGANer\n",
    "from tqdm import tqdm\n",
    "from utils import *\n",
    "\n",
    "from concrete.ml.torch.hybrid_model import HybridFHEModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose \n",
    "\n",
    "The purpose of this notebook is to compute, for an input image of size 3 * 256 * 256:\n",
    "- the transmission latency \n",
    "- data size of input and output tensors for each intermediate layer of the model (Conv2d and Linear) \n",
    "\n",
    "\n",
    "### Overview of the GFP-GAN Architecture\n",
    "\n",
    "The GFP-GAN pipeline is divided into 3 main components:\n",
    "\n",
    "1. ***Face Cropping (restorer.face_helper):*** To detect and crop faces from input images.\n",
    "\n",
    "   Composition:\n",
    "   - 82 Standard Convolutional Layers with:\n",
    "        + Kernel Sizes: (7, 7) or (3, 3) or (1, 1)\n",
    "        + Strides: (2, 2) or (1, 1)\n",
    "        + Padding: (3, 3) or (1, 1)\n",
    "\n",
    "2. ***Face Restoration (restorer.gfpgan):*** To restore and enhance the quality of cropped facial images.\n",
    "\n",
    "   Composition:\n",
    "   - 32 Linear Layers\n",
    "   - 79 Standard Convolutional Layers with:\n",
    "        + Kernel Sizes: (3, 3) or (1, 1)\n",
    "        + Strides: (1, 1)\n",
    "        + Padding: (1, 1)\n",
    "   - 23 Modulated Convolutional Layers (ModulatedConv2d), with: Kernel Sizes: 3 or 1\n",
    "\n",
    "\n",
    "3. **Background Enhancement (restorer.upsampler):** To enhance the background details of the images after face restoration.\n",
    "\n",
    "   Composition:\n",
    "   - 351 Standard Convolutional Layers with fixed configurations:\n",
    "        + Kernel Size: (3, 3)\n",
    "        + Stride: (1, 1)\n",
    "        + Padding: (1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Absence of Grouped or Dilated Convolutions or Depthwise Convolutions**\n",
    "\n",
    "+ Grouped Convolutions: Convolutions where the input is divided into parts/groups, we have a set of filters for each group, the result is concatenated.\n",
    "(groups=1 by default).\n",
    "\n",
    "+ Dilated Convolutions: Convolutions where the kernel is expanded by inserting zeros between its elements, increasing the receptive field without increasing the number of parameters.\n",
    "(dilation=1 by default).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modulated**:\n",
    "\n",
    "+ The convolutional weights are dynamically adjusted (modulated) for each input sample based on a style vector.\n",
    "+ This modulation allows the network to adapt its convolutional filters per sample, enabling more control over generated features.\n",
    "+ Modulate Weights Process: For each ModulatedConv2d layer:\n",
    "        - The style vector is transformed (usually via another linear layer) to obtain modulation weights.\n",
    "        - These weights modulate the convolutional filters.\n",
    "        - Demodulation: After modulation, weights can vary in magnitude, leading to instability during training. Demodulation normalizes the weights to maintain a consistent signal magnitude across the layers.\n",
    "\n",
    "\n",
    "```\n",
    "Style Vector (w)\n",
    "        |\n",
    "Modulation Weights (s)\n",
    "        |\n",
    "Modulated Weights (s * k)\n",
    "        |\n",
    "(Optional) Demodulation\n",
    "        |\n",
    "Convolution Operation\n",
    "        |\n",
    "Output Feature Maps\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall: H_out​ = ⌊​H_in​ + 2 × P_h​ − D_h ​* (K_h ​− 1) − 1 ⌋ / S_h ​+ 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.input = \"GFPGAN/inputs/whole_imgs\"\n",
    "        self.output = \"results\"\n",
    "        self.version = \"1.4\"\n",
    "        self.upscale = 5\n",
    "        self.bg_upsampler = \"realesrgan\"\n",
    "        self.bg_tile = 400\n",
    "        self.suffix = None\n",
    "        self.only_center_face = False\n",
    "        self.aligned = False\n",
    "        self.ext = \"auto\"\n",
    "        self.weight = 0.5\n",
    "\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_background_improvement = True\n",
    "\n",
    "if args.bg_upsampler == \"realesrgan\":\n",
    "    if use_background_improvement:\n",
    "\n",
    "        half = True if torch.cuda.is_available() else False\n",
    "\n",
    "        model = RRDBNet(\n",
    "            num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=2\n",
    "        )\n",
    "        # No linear modules in this model\n",
    "        bg_upsampler = RealESRGANer(\n",
    "            scale=2,  # Do not change this value\n",
    "            model_path=\"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth\",\n",
    "            model=model,\n",
    "            tile=args.bg_tile,\n",
    "            tile_pad=10,\n",
    "            pre_pad=0,\n",
    "            half=half,\n",
    "        )  # need to set False in CPU mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.version == \"1.3\":\n",
    "    arch = \"clean\"\n",
    "    channel_multiplier = 2\n",
    "    model_name = \"GFPGANv1.3\"\n",
    "    url = \"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth\"\n",
    "    local_model_path = \"GFPGANv1.3.pth\"\n",
    "elif args.version == \"1.4\":\n",
    "    arch = \"clean\"\n",
    "    channel_multiplier = 2\n",
    "    model_name = \"GFPGANv1.4\"\n",
    "    url = \"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth\"\n",
    "    local_model_path = \"GFPGANv1.4.pth\"\n",
    "\n",
    "# determine model paths\n",
    "model_path = os.path.join(\"experiments/pretrained_models\", model_name + \".pth\")\n",
    "if not os.path.isfile(model_path):\n",
    "    model_path = os.path.join(\"gfpgan/weights\", model_name + \".pth\")\n",
    "if not os.path.isfile(model_path):\n",
    "    # download pre-trained models from url\n",
    "    model_path = url\n",
    "\n",
    "restorer = GFPGANer(\n",
    "    model_path=model_path,\n",
    "    upscale=args.upscale,\n",
    "    arch=arch,\n",
    "    channel_multiplier=channel_multiplier,\n",
    "    bg_upsampler=bg_upsampler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BYTES_PER_VALUE = 2  # For 16-bit precision\n",
    "EXPANSION_FACTOR = 5\n",
    "DATA_TRANSMISSION = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_data_bytes_for_selected_layer_class(layer_shapes, selected_class_name):\n",
    "\n",
    "    total_data = 0\n",
    "\n",
    "    for info in layer_shapes:\n",
    "        class_name = info[\"class_name\"]\n",
    "        layer_name = info[\"layer_name\"]\n",
    "\n",
    "        if class_name == selected_class_name:\n",
    "            input_shapes = info[\"input_shapes\"]\n",
    "            output_shapes = info[\"output_shapes\"]\n",
    "\n",
    "            try:\n",
    "                C_in, H_in, W_in = extract_dimensions(input_shapes, layer_name)\n",
    "                C_out, H_out, W_out = extract_dimensions(output_shapes, layer_name)\n",
    "\n",
    "                input_size = C_in * H_in * W_in\n",
    "                output_size = C_out * H_out * W_out\n",
    "\n",
    "                layer_data = input_size + output_size\n",
    "                total_data += layer_data\n",
    "\n",
    "            except ValueError as e:\n",
    "                print(\"Error:\", e)\n",
    "\n",
    "    print(\"\\n\\n===================================================================\")\n",
    "\n",
    "    print(\n",
    "        f\"Sum of input and output elements for all '{selected_class_name}' layers): {total_data} elements\\n\"\n",
    "    )\n",
    "\n",
    "    # Data transmission\n",
    "    total_data *= DATA_TRANSMISSION\n",
    "    print(\n",
    "        f\"Total data after accounting for data transmission (x{DATA_TRANSMISSION}): {total_data} elements\\n\"\n",
    "    )\n",
    "\n",
    "    total_data *= EXPANSION_FACTOR\n",
    "    print(\n",
    "        f\"Total data after applying expansion factor (x{EXPANSION_FACTOR}): {total_data} elements\\n\"\n",
    "    )\n",
    "\n",
    "    total_data_bytes = total_data * BYTES_PER_VALUE\n",
    "    print(\n",
    "        f\"Total data in bytes (assuming {BYTES_PER_VALUE}-byte precision per value): {total_data_bytes} bytes\\n\"\n",
    "    )\n",
    "\n",
    "    # Convert to MB and GB\n",
    "    total_data_mb = total_data_bytes / (1024**2)\n",
    "    total_data_gb = total_data_bytes / (1024**3)\n",
    "\n",
    "    print(f\"Total data size:\")\n",
    "    print(f\"  -{total_data_bytes} bytes\")\n",
    "    print(f\"  -{total_data_mb:.2f} MB\")\n",
    "    print(f\"  -{total_data_gb:.2f} GB\")\n",
    "\n",
    "    return total_data_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_network_size(layer_shapes):\n",
    "\n",
    "    total_data = 0\n",
    "    total_params = 0\n",
    "\n",
    "    for info in layer_shapes:\n",
    "        class_name = info[\"class_name\"]\n",
    "        layer_name = info[\"module\"].__class__.__name__\n",
    "        module = info[\"module\"]\n",
    "\n",
    "        input_shapes = info[\"input_shapes\"]\n",
    "        output_shapes = info[\"output_shapes\"]\n",
    "\n",
    "        try:\n",
    "            C_in, H_in, W_in = extract_dimensions(input_shapes, layer_name)\n",
    "            input_size = C_in * H_in * W_in\n",
    "\n",
    "            C_out, H_out, W_out = extract_dimensions(output_shapes, layer_name)\n",
    "            output_size = C_out * H_out * W_out\n",
    "\n",
    "            params_size = 0\n",
    "            for param in module.parameters():\n",
    "                params_size += param.numel()\n",
    "\n",
    "            layer_data = input_size + output_size + params_size\n",
    "            total_data += layer_data\n",
    "            total_params += params_size\n",
    "\n",
    "            # print(f\"Layer: {layer_name} ({class_name})\")\n",
    "            # print(f\"  Input size: {input_size}\")\n",
    "            # print(f\"  Output size: {output_size}\")\n",
    "            # print(f\"  Parameters size: {params_size}\")\n",
    "            # print(f\"  Total layer size: {layer_data}\\n\")\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(f\"Error processing layer {layer_name}: {e}\")\n",
    "\n",
    "    total_network_size = total_data + total_params\n",
    "\n",
    "    total_memory_bytes = total_network_size * BYTES_PER_VALUE\n",
    "\n",
    "    # Convert to MB and GB\n",
    "    total_memory_mb = total_memory_bytes / (1024**2)\n",
    "    total_memory_gb = total_memory_bytes / (1024**3)\n",
    "\n",
    "    print(\"\\n\\n===================================================================\")\n",
    "\n",
    "    print(f\"Total network size in bytes: {total_memory_bytes} bytes\")\n",
    "    print(f\"Total network size in megabytes: {total_memory_mb:.2f} MB\")\n",
    "    print(f\"Total network size in megabytes: {total_memory_gb:.2f} GB\")\n",
    "\n",
    "    return total_network_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define an input of size : 3, 256, 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.randn(1, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - Face cropping and extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-Linear Layers, 82-Conv Layers\n"
     ]
    }
   ],
   "source": [
    "face_helper_model = restorer.face_helper.face_det\n",
    "\n",
    "restorer_face_helper_linear = extract_specific_module(\n",
    "    face_helper_model, dtype_layer=torch.nn.Linear, verbose=False\n",
    ")\n",
    "restorer_face_helper_conv2d = extract_specific_module(\n",
    "    face_helper_model, dtype_layer=torch.nn.Conv2d, verbose=False\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"{len(restorer_face_helper_linear)}-Linear Layers, \"\n",
    "    f\"{len(restorer_face_helper_conv2d)}-Conv Layers\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Summary ===================================================================\n",
      "Layer: body.conv1 - Conv2d\n",
      "  Input shapes: [torch.Size([1, 3, 256, 256])]\n",
      "  Output shapes: torch.Size([1, 64, 128, 128])\n",
      "\n",
      "Layer: body.bn1 - BatchNorm2d\n",
      "  Input shapes: [torch.Size([1, 64, 128, 128])]\n",
      "  Output shapes: torch.Size([1, 64, 128, 128])\n",
      "\n",
      "Layer: body.relu - ReLU\n",
      "  Input shapes: [torch.Size([1, 64, 128, 128])]\n",
      "  Output shapes: torch.Size([1, 64, 128, 128])\n",
      "\n",
      "Layer: body.maxpool - MaxPool2d\n",
      "  Input shapes: [torch.Size([1, 64, 128, 128])]\n",
      "  Output shapes: torch.Size([1, 64, 64, 64])\n",
      "\n",
      "Layer: body.layer1.0.conv1 - Conv2d\n",
      "  Input shapes: [torch.Size([1, 64, 64, 64])]\n",
      "  Output shapes: torch.Size([1, 64, 64, 64])\n",
      "\n",
      "Layer: body.layer1.0.bn1 - BatchNorm2d\n",
      "  Input shapes: [torch.Size([1, 64, 64, 64])]\n",
      "  Output shapes: torch.Size([1, 64, 64, 64])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the input and output shapes for each layer\n",
    "layer_shapes = custom_torch_summary(face_helper_model, input_tensor, verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing layer IntermediateLayerGetter: Unexpected shape {1: torch.Size([1, 512, 32, 32]), 2: torch.Size([1, 1024, 16, 16]), 3: torch.Size([1, 2048, 8, 8])} in layer IntermediateLayerGetter\n",
      "Error processing layer FPN: not enough values to unpack (expected 4, got 3)\n",
      "Error processing layer BboxHead: Unexpected shape torch.Size([1, 2048, 4]) in layer BboxHead\n",
      "Error processing layer BboxHead: Unexpected shape torch.Size([1, 512, 4]) in layer BboxHead\n",
      "Error processing layer BboxHead: Unexpected shape torch.Size([1, 128, 4]) in layer BboxHead\n",
      "Error processing layer ClassHead: Unexpected shape torch.Size([1, 2048, 2]) in layer ClassHead\n",
      "Error processing layer ClassHead: Unexpected shape torch.Size([1, 512, 2]) in layer ClassHead\n",
      "Error processing layer ClassHead: Unexpected shape torch.Size([1, 128, 2]) in layer ClassHead\n",
      "Error processing layer LandmarkHead: Unexpected shape torch.Size([1, 2048, 10]) in layer LandmarkHead\n",
      "Error processing layer LandmarkHead: Unexpected shape torch.Size([1, 512, 10]) in layer LandmarkHead\n",
      "Error processing layer LandmarkHead: Unexpected shape torch.Size([1, 128, 10]) in layer LandmarkHead\n",
      "\n",
      "\n",
      "===================================================================\n",
      "Total network size in bytes: 568249984 bytes\n",
      "Total network size in megabytes: 541.93 MB\n",
      "Total network size in megabytes: 0.53 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "284124992"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_network_size = compute_total_network_size(layer_shapes)\n",
    "total_network_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===================================================================\n",
      "Sum of input and output elements for all 'Conv2d' layers): 32897024 elements\n",
      "\n",
      "Total data after accounting for data transmission (x2): 65794048 elements\n",
      "\n",
      "Total data after applying expansion factor (x5): 328970240 elements\n",
      "\n",
      "Total data in bytes (assuming 2-byte precision per value): 657940480 bytes\n",
      "\n",
      "Total data size:\n",
      "  -657940480 bytes\n",
      "  -627.46 MB\n",
      "  -0.61 GB\n"
     ]
    }
   ],
   "source": [
    "_ = compute_total_data_bytes_for_selected_layer_class(layer_shapes, selected_class_name=\"Conv2d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - Face Restoration (restorer.gfpgan):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32-Linear Layers, 79-Conv Layers, 23-Conv Modulated Layers\n"
     ]
    }
   ],
   "source": [
    "restorer_gfpgan = restorer.gfpgan\n",
    "\n",
    "restorer_gfpgan_linear = extract_specific_module(\n",
    "    restorer_gfpgan, dtype_layer=torch.nn.Linear, verbose=False\n",
    ")\n",
    "\n",
    "restorer_gfpgan_conv2d = extract_specific_module(\n",
    "    restorer_gfpgan, dtype_layer=torch.nn.Conv2d, verbose=False\n",
    ")\n",
    "\n",
    "restorer_gfpgan_modulated_conv2d = extract_specific_module(\n",
    "    restorer_gfpgan, dtype_layer=ModulatedConv2d, verbose=False\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"{len(restorer_gfpgan_linear)}-Linear Layers, \"\n",
    "    f\"{len(restorer_gfpgan_conv2d)}-Conv Layers, \"\n",
    "    f\"{len(restorer_gfpgan_modulated_conv2d)}-Conv Modulated Layers\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple,\n",
       " 3,\n",
       " [torch.Size([1, 2688, 4]),\n",
       "  torch.Size([1, 2688, 2]),\n",
       "  torch.Size([1, 2688, 10])])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox = face_helper_model(input_tensor)\n",
    "\n",
    "type(bbox), len(bbox), [(b.shape) for b in bbox]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing module ConstantInput(): Couldn't deal with layer 'stylegan_decoder.constant_input': unrecognized data type <class 'int'>\n",
      "Input type: <class 'tuple'>, Output type: <class 'torch.Tensor'>\n",
      "Error processing module StyleGAN2GeneratorCSFT(\n",
      "  (style_mlp): Sequential(\n",
      "    (0): NormStyleCode()\n",
      "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (6): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (7): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (9): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (12): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (13): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (14): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (15): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (16): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (constant_input): ConstantInput()\n",
      "  (style_conv1): StyleConv(\n",
      "    (modulated_conv): ModulatedConv2d(in_channels=512, out_channels=512, kernel_size=3, demodulate=True, sample_mode=None)\n",
      "    (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (to_rgb1): ToRGB(\n",
      "    (modulated_conv): ModulatedConv2d(in_channels=512, out_channels=3, kernel_size=1, demodulate=False, sample_mode=None)\n",
      "  )\n",
      "  (style_convs): ModuleList(\n",
      "    (0): StyleConv(\n",
      "      (modulated_conv): ModulatedConv2d(in_channels=512, out_channels=512, kernel_size=3, demodulate=True, sample_mode=upsample)\n",
      "      (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (1): StyleConv(\n",
      "      (modulated_conv): ModulatedConv2d(in_channels=512, out_channels=512, kernel_size=3, demodulate=True, sample_mode=None)\n",
      "      (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (2): StyleConv(\n",
      "      (modulated_conv): ModulatedConv2d(in_channels=512, out_channels=512, kernel_size=3, demodulate=True, sample_mode=upsample)\n",
      "      (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (3): StyleConv(\n",
      "      (modulated_conv): ModulatedConv2d(in_channels=512, out_channels=512, kernel_size=3, demodulate=True, sample_mode=None)\n",
      "      (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (4): StyleConv(\n",
      "      (modulated_conv): ModulatedConv2d(in_channels=512, out_channels=512, kernel_size=3, demodulate=True, sample_mode=upsample)\n",
      "      (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (5): StyleConv(\n",
      "      (modulated_conv): ModulatedConv2d(in_channels=512, out_channels=512, kernel_size=3, demodulate=True, sample_mode=None)\n",
      "      (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (6): StyleConv(\n",
      "      (modulated_conv): ModulatedConv2d(in_channels=512, out_channels=512, kernel_size=3, demodulate=True, sample_mode=upsample)\n",
      "      (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (7): StyleConv(\n",
      "      (modulated_conv): ModulatedConv2d(in_channels=512, out_channels=512, kernel_size=3, demodulate=True, sample_mode=None)\n",
      "      (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (8): StyleConv(\n",
      "      (modulated_conv): ModulatedConv2d(in_channels=512, out_channels=256, kernel_size=3, demodulate=True, sample_mode=upsample)\n",
      "      (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (9): StyleConv(\n",
      "      (modulated_conv): ModulatedConv2d(in_channels=256, out_channels=256, kernel_size=3, demodulate=True, sample_mode=None)\n",
      "      (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (10): StyleConv(\n",
      "      (modulated_conv): ModulatedConv2d(in_channels=256, out_channels=128, kernel_size=3, demodulate=True, sample_mode=upsample)\n",
      "      (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (11): StyleConv(\n",
      "      (modulated_conv): ModulatedConv2d(in_channels=128, out_channels=128, kernel_size=3, demodulate=True, sample_mode=None)\n",
      "      (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (12): StyleConv(\n",
      "      (modulated_conv): ModulatedConv2d(in_channels=128, out_channels=64, kernel_size=3, demodulate=True, sample_mode=upsample)\n",
      "      (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "    (13): StyleConv(\n",
      "      (modulated_conv): ModulatedConv2d(in_channels=64, out_channels=64, kernel_size=3, demodulate=True, sample_mode=None)\n",
      "      (activate): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (to_rgbs): ModuleList(\n",
      "    (0-3): 4 x ToRGB(\n",
      "      (modulated_conv): ModulatedConv2d(in_channels=512, out_channels=3, kernel_size=1, demodulate=False, sample_mode=None)\n",
      "    )\n",
      "    (4): ToRGB(\n",
      "      (modulated_conv): ModulatedConv2d(in_channels=256, out_channels=3, kernel_size=1, demodulate=False, sample_mode=None)\n",
      "    )\n",
      "    (5): ToRGB(\n",
      "      (modulated_conv): ModulatedConv2d(in_channels=128, out_channels=3, kernel_size=1, demodulate=False, sample_mode=None)\n",
      "    )\n",
      "    (6): ToRGB(\n",
      "      (modulated_conv): ModulatedConv2d(in_channels=64, out_channels=3, kernel_size=1, demodulate=False, sample_mode=None)\n",
      "    )\n",
      "  )\n",
      "  (noises): Module()\n",
      "): Couldn't deal with layer 'stylegan_decoder': unrecognized data type <class 'NoneType'>\n",
      "Input type: <class 'tuple'>, Output type: <class 'tuple'>\n",
      "\n",
      "\n",
      "Summary ===================================================================\n",
      "Layer: conv_body_first - Conv2d\n",
      "  Input shapes: [torch.Size([1, 3, 512, 512])]\n",
      "  Output shapes: torch.Size([1, 32, 512, 512])\n",
      "\n",
      "Layer: conv_body_down.0.conv1 - Conv2d\n",
      "  Input shapes: [torch.Size([1, 32, 512, 512])]\n",
      "  Output shapes: torch.Size([1, 32, 512, 512])\n",
      "\n",
      "Layer: conv_body_down.0.conv2 - Conv2d\n",
      "  Input shapes: [torch.Size([1, 32, 256, 256])]\n",
      "  Output shapes: torch.Size([1, 64, 256, 256])\n",
      "\n",
      "Layer: conv_body_down.0.skip - Conv2d\n",
      "  Input shapes: [torch.Size([1, 32, 256, 256])]\n",
      "  Output shapes: torch.Size([1, 64, 256, 256])\n",
      "\n",
      "Layer: conv_body_down.0 - ResBlock\n",
      "  Input shapes: [torch.Size([1, 32, 512, 512])]\n",
      "  Output shapes: torch.Size([1, 64, 256, 256])\n",
      "\n",
      "Layer: conv_body_down.1.conv1 - Conv2d\n",
      "  Input shapes: [torch.Size([1, 64, 256, 256])]\n",
      "  Output shapes: torch.Size([1, 64, 256, 256])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# <!!!> restorer_gfpgan takes 3 * 512 * 512 input for each detected face\n",
    "\n",
    "layer_shapes = custom_torch_summary(restorer_gfpgan, torch.randn(1, 3, 512, 512), verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===================================================================\n",
      "Sum of input and output elements for all 'Conv2d' layers): 319471552 elements\n",
      "\n",
      "Total data after accounting for data transmission (x2): 638943104 elements\n",
      "\n",
      "Total data after applying expansion factor (x5): 3194715520 elements\n",
      "\n",
      "Total data in bytes (assuming 2-byte precision per value): 6389431040 bytes\n",
      "\n",
      "Total data size:\n",
      "  -6389431040 bytes\n",
      "  -6093.44 MB\n",
      "  -5.95 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6389431040"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_data = compute_total_data_bytes_for_selected_layer_class(\n",
    "    layer_shapes, selected_class_name=\"Conv2d\"\n",
    ")\n",
    "total_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 - Background Enhancement (restorer.upsampler):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-Linear Layers, 351-Conv Layers\n"
     ]
    }
   ],
   "source": [
    "upsampler_model = restorer.bg_upsampler.model\n",
    "\n",
    "restorer_upsampler_linear = extract_specific_module(\n",
    "    upsampler_model, dtype_layer=torch.nn.Linear, verbose=False\n",
    ")\n",
    "restorer_upsampler_conv2d = extract_specific_module(\n",
    "    upsampler_model, dtype_layer=torch.nn.Conv2d, verbose=False\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"{len(restorer_upsampler_linear)}-Linear Layers, \"\n",
    "    f\"{len(restorer_upsampler_conv2d)}-Conv Layers\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "\n",
    "# summary(upsampler_model, input_tensor.squeeze().shape)\n",
    "\n",
    "# # <!!!!> Saved in upsampler_sizes_v2.txt and upsampler_sizes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"upsampler_sizes_v2.txt\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "previous_output_shape = input_tensor.shape\n",
    "\n",
    "parsed_data = [parse_line(line) for line in data][1:]\n",
    "\n",
    "formatted_data = reformat_data(parsed_data, previous_output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===================================================================\n",
      "Sum of input and output elements for all 'Conv2d' layers): 514785280 elements\n",
      "\n",
      "Total data after accounting for data transmission (x2): 1029570560 elements\n",
      "\n",
      "Total data after applying expansion factor (x5): 5147852800 elements\n",
      "\n",
      "Total data in bytes (assuming 2-byte precision per value): 10295705600 bytes\n",
      "\n",
      "Total data size:\n",
      "  -10295705600 bytes\n",
      "  -9818.75 MB\n",
      "  -9.59 GB\n"
     ]
    }
   ],
   "source": [
    "total_data = compute_total_data_bytes_for_selected_layer_class(\n",
    "    formatted_data, selected_class_name=\"Conv2d\"\n",
    ")\n",
    "\n",
    "# Sanity check only\n",
    "assert (\n",
    "    total_data\n",
    "    == filter_conv_layers(data, (3, 256, 256))[1]\n",
    "    * DATA_TRANSMISSION\n",
    "    * EXPANSION_FACTOR\n",
    "    * BYTES_PER_VALUE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.36"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generalization\n",
    "\n",
    "ref_w = ref_h = 256\n",
    "new_w = new_h = 512\n",
    "\n",
    "(new_h * new_w) / (ref_w * ref_h) * 9.59"
   ]
  }
 ],
 "metadata": {
  "execution": {
   "timeout": 10800
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
