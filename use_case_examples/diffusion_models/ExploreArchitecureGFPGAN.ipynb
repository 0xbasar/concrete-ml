{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/celia/Desktop/Zama/concrete-internal/.dm/lib/python3.8/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "from copy import copy\n",
    "from pprint import pprint\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "from basicsr.utils import imwrite\n",
    "from gfpgan import GFPGANer\n",
    "from gfpgan.archs.stylegan2_clean_arch import ModulatedConv2d\n",
    "from realesrgan import RealESRGANer\n",
    "from tqdm import tqdm\n",
    "from utils import *\n",
    "\n",
    "from concrete.ml.torch.hybrid_model import HybridFHEModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of the GFP-GAN Architecture\n",
    "\n",
    "The GFP-GAN pipeline is divided into 3 main components:\n",
    "\n",
    "1. Face Cropping (restorer.face_helper): To detect and crop faces from input images.\n",
    "\n",
    "    Composition:\n",
    "    - 82 Standard Convolutional Layers with:\n",
    "        + Kernel Sizes: (7, 7) or (3, 3) or (1, 1)\n",
    "        + Strides: (2, 2) or (1, 1)\n",
    "        + Padding: (3, 3) or (1, 1)\n",
    "\n",
    "\n",
    "    - **Absence of Grouped or Dilated Convolutions or Depthwise Convolutions**\n",
    "\n",
    "        + Grouped Convolutions: Convolutions where the input is divided into parts/groups, we have a set of filters for each group, the result is concatenated.\n",
    "        (groups=1 by default).\n",
    "\n",
    "        + Dilated Convolutions: Convolutions where the kernel is expanded by inserting zeros between its elements, increasing the receptive field without increasing the number of parameters.\n",
    "        (dilation=1 by default).\n",
    "\n",
    "    - **Modulated**:\n",
    "\n",
    "        + The convolutional weights are dynamically adjusted (modulated) for each input sample based on a style vector.\n",
    "        + This modulation allows the network to adapt its convolutional filters per sample, enabling more control over generated features.\n",
    "        + Modulate Weights Process: For each ModulatedConv2d layer:\n",
    "            - The style vector is transformed (usually via another linear layer) to obtain modulation weights.\n",
    "            - These weights modulate the convolutional filters.\n",
    "            - Demodulation: After modulation, weights can vary in magnitude, leading to instability during training. Demodulation normalizes the weights to maintain a consistent signal magnitude across the layers.\n",
    "\n",
    "\n",
    "        ```python\n",
    "        Style Vector (w)\n",
    "                |\n",
    "        Modulation Weights (s)\n",
    "                |\n",
    "        Modulated Weights (s * k)\n",
    "                |\n",
    "        (Optional) Demodulation\n",
    "                |\n",
    "        Convolution Operation\n",
    "                |\n",
    "        Output Feature Maps\n",
    "        ```\n",
    "\n",
    "2. Face Restoration (restorer.gfpgan): To restore and enhance the quality of cropped facial images.\n",
    "\n",
    "    Composition:\n",
    "    - 32 Linear Layers\n",
    "    - 79 Standard Convolutional Layers with:\n",
    "        + Kernel Sizes: (3, 3) or (1, 1)\n",
    "        + Strides: (1, 1)\n",
    "        + Padding: (1, 1)\n",
    "    - 23 Modulated Convolutional Layers (ModulatedConv2d), with: Kernel Sizes: 3 or 11\n",
    "\n",
    "3. Background Enhancement (restorer.upsampler): To enhance the background details of the images after face restoration.\n",
    "\n",
    "    Composition:\n",
    "    - 351 Standard Convolutional Layers with fixed configurations:\n",
    "        + Kernel Size: (3, 3)\n",
    "        + Stride: (1, 1)\n",
    "        + Padding: (1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H_out​ = ⌊​H_in​ + 2 × P_h​ − D_h ​* (K_h ​− 1) − 1 ⌋ / S_h ​+ 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.input = \"GFPGAN/inputs/whole_imgs\"\n",
    "        self.output = \"results\"\n",
    "        self.version = \"1.4\"\n",
    "        self.upscale = 5\n",
    "        self.bg_upsampler = \"realesrgan\"\n",
    "        self.bg_tile = 400\n",
    "        self.suffix = None\n",
    "        self.only_center_face = False\n",
    "        self.aligned = False\n",
    "        self.ext = \"auto\"\n",
    "        self.weight = 0.5\n",
    "\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_background_improvement = True\n",
    "\n",
    "if args.bg_upsampler == \"realesrgan\":\n",
    "    if use_background_improvement:\n",
    "\n",
    "        half = True if torch.cuda.is_available() else False\n",
    "\n",
    "        model = RRDBNet(\n",
    "            num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=2\n",
    "        )\n",
    "        # No linear modules in this model\n",
    "        bg_upsampler = RealESRGANer(\n",
    "            scale=2,  # Do not change this value\n",
    "            model_path=\"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth\",\n",
    "            model=model,\n",
    "            tile=args.bg_tile,\n",
    "            tile_pad=10,\n",
    "            pre_pad=0,\n",
    "            half=half,\n",
    "        )  # need to set False in CPU mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.version == \"1.3\":\n",
    "    arch = \"clean\"\n",
    "    channel_multiplier = 2\n",
    "    model_name = \"GFPGANv1.3\"\n",
    "    url = \"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth\"\n",
    "    local_model_path = \"GFPGANv1.3.pth\"\n",
    "elif args.version == \"1.4\":\n",
    "    arch = \"clean\"\n",
    "    channel_multiplier = 2\n",
    "    model_name = \"GFPGANv1.4\"\n",
    "    url = \"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth\"\n",
    "    local_model_path = \"GFPGANv1.4.pth\"\n",
    "\n",
    "# determine model paths\n",
    "model_path = os.path.join(\"experiments/pretrained_models\", model_name + \".pth\")\n",
    "if not os.path.isfile(model_path):\n",
    "    model_path = os.path.join(\"gfpgan/weights\", model_name + \".pth\")\n",
    "if not os.path.isfile(model_path):\n",
    "    # download pre-trained models from url\n",
    "    model_path = url\n",
    "\n",
    "restorer = GFPGANer(\n",
    "    model_path=model_path,\n",
    "    upscale=args.upscale,\n",
    "    arch=arch,\n",
    "    channel_multiplier=channel_multiplier,\n",
    "    bg_upsampler=bg_upsampler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_data(layer_shapes, selected_class_name):\n",
    "\n",
    "    total_data = 0\n",
    "    for layer_name, info in layer_shapes.items():\n",
    "        class_name = info[\"class_name\"]\n",
    "\n",
    "        if class_name == selected_class_name:\n",
    "            input_shapes = info[\"input_shapes\"]\n",
    "            output_shapes = info[\"output_shapes\"]\n",
    "\n",
    "            try:\n",
    "                C_in, H_in, W_in = extract_dimensions(input_shapes, layer_name)\n",
    "                C_out, H_out, W_out = extract_dimensions(output_shapes, layer_name)\n",
    "\n",
    "                input_size = C_in * H_in * W_in\n",
    "                output_size = C_out * H_out * W_out\n",
    "\n",
    "                layer_data = input_size + output_size\n",
    "                total_data += layer_data\n",
    "\n",
    "            except ValueError as e:\n",
    "                print(e)\n",
    "\n",
    "    return total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sample input tensor\n",
    "input_tensor = torch.randn(1, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - Face cropping and extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-Linear Layers, 82-Conv Layers\n"
     ]
    }
   ],
   "source": [
    "face_helper_model = restorer.face_helper.face_det\n",
    "\n",
    "restorer_face_helper_linear = extract_specific_module(\n",
    "    face_helper_model, dtype_layer=torch.nn.Linear, verbose=False\n",
    ")\n",
    "restorer_face_helper_conv2d = extract_specific_module(\n",
    "    face_helper_model, dtype_layer=torch.nn.Conv2d, verbose=False\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"{len(restorer_face_helper_linear)}-Linear Layers, {len(restorer_face_helper_conv2d)}-Conv Layers\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: body.conv1 - Conv2d\n",
      "  Input shapes: [torch.Size([1, 3, 256, 256])]\n",
      "  Output shapes: torch.Size([1, 64, 128, 128])\n",
      "\n",
      "Layer: body.bn1 - BatchNorm2d\n",
      "  Input shapes: [torch.Size([1, 64, 128, 128])]\n",
      "  Output shapes: torch.Size([1, 64, 128, 128])\n",
      "\n",
      "Layer: body.relu - ReLU\n",
      "  Input shapes: [torch.Size([1, 64, 128, 128])]\n",
      "  Output shapes: torch.Size([1, 64, 128, 128])\n",
      "\n",
      "Layer: body.maxpool - MaxPool2d\n",
      "  Input shapes: [torch.Size([1, 64, 128, 128])]\n",
      "  Output shapes: torch.Size([1, 64, 64, 64])\n",
      "\n",
      "Layer: body.layer1.0.conv1 - Conv2d\n",
      "  Input shapes: [torch.Size([1, 64, 64, 64])]\n",
      "  Output shapes: torch.Size([1, 64, 64, 64])\n",
      "\n",
      "Layer: body.layer1.0.bn1 - BatchNorm2d\n",
      "  Input shapes: [torch.Size([1, 64, 64, 64])]\n",
      "  Output shapes: torch.Size([1, 64, 64, 64])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the input and output shapes for each layer\n",
    "layer_shapes = custom_torch_summary(face_helper_model, input_tensor, verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_shapes\n",
    "total_data = compute_total_data(layer_shapes, selected_class_name=\"Conv2d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65794048"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data transmission\n",
    "total_data *= 2\n",
    "total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328970240\n",
      "657940480 bytes\n",
      "627.46 MB\n",
      "0.61 GB\n"
     ]
    }
   ],
   "source": [
    "expansion_factor = 5\n",
    "total_data *= expansion_factor\n",
    "print(total_data)\n",
    "\n",
    "bytes_per_value = 2  # For 16-bit precision\n",
    "total_data_bytes = total_data * bytes_per_value\n",
    "\n",
    "# Convert to MB and GB\n",
    "total_data_mb = total_data_bytes / (1024**2)\n",
    "total_data_gb = total_data_bytes / (1024**3)\n",
    "\n",
    "print(f\"{total_data_bytes} bytes\")\n",
    "print(f\"{total_data_mb:.2f} MB\")\n",
    "print(f\"{total_data_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32-Linear Layers, 79-Conv Layers, 23-Conv Modulated Layers\n"
     ]
    }
   ],
   "source": [
    "restorer_gfpgan = restorer.gfpgan\n",
    "\n",
    "restorer_gfpgan_linear = extract_specific_module(\n",
    "    restorer_gfpgan, dtype_layer=torch.nn.Linear, verbose=False\n",
    ")\n",
    "\n",
    "restorer_gfpgan_conv2d = extract_specific_module(\n",
    "    restorer_gfpgan, dtype_layer=torch.nn.Conv2d, verbose=False\n",
    ")\n",
    "\n",
    "restorer_gfpgan_modulated_conv2d = extract_specific_module(\n",
    "    restorer_gfpgan, dtype_layer=ModulatedConv2d, verbose=False\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"{len(restorer_gfpgan_linear)}-Linear Layers, \"\n",
    "    f\"{len(restorer_gfpgan_conv2d)}-Conv Layers, \"\n",
    "    f\"{len(restorer_gfpgan_modulated_conv2d)}-Conv Modulated Layers\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the input and output shapes for each layer\n",
    "# layer_shapes = custom_torch_summary(restorer_gfpgan, input_tensor, verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-Linear Layers, 351-Conv Layers\n"
     ]
    }
   ],
   "source": [
    "upsampler_model = restorer.bg_upsampler.model\n",
    "\n",
    "restorer_upsampler_linear = extract_specific_module(\n",
    "    upsampler_model, dtype_layer=torch.nn.Linear, verbose=False\n",
    ")\n",
    "restorer_upsampler_conv2d = extract_specific_module(\n",
    "    upsampler_model, dtype_layer=torch.nn.Conv2d, verbose=False\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"{len(restorer_upsampler_linear)}-Linear Layers, \"\n",
    "    f\"{len(restorer_upsampler_conv2d)}-Conv Layers\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "\n",
    "# summary(upsampler_model, input_tensor.squeeze().shape)\n",
    "\n",
    "# # <!!!!> Saved in upsampler_sizes_v2.txt and upsampler_sizes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'class_name': 'Conv2d-1',\n",
       "   'input_shape': torch.Size([3, 256, 256]),\n",
       "   'output_shape': (64, 128, 128),\n",
       "   'total_size': 1245184},\n",
       "  {'class_name': 'Conv2d-2',\n",
       "   'input_shape': (64, 128, 128),\n",
       "   'output_shape': (32, 128, 128),\n",
       "   'total_size': 1572864},\n",
       "  {'class_name': 'Conv2d-4',\n",
       "   'input_shape': (32, 128, 128),\n",
       "   'output_shape': (32, 128, 128),\n",
       "   'total_size': 1048576},\n",
       "  {'class_name': 'Conv2d-6',\n",
       "   'input_shape': (32, 128, 128),\n",
       "   'output_shape': (32, 128, 128),\n",
       "   'total_size': 1048576},\n",
       "  {'class_name': 'Conv2d-8',\n",
       "   'input_shape': (32, 128, 128),\n",
       "   'output_shape': (32, 128, 128),\n",
       "   'total_size': 1048576}],\n",
       " 514785280)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"upsampler_sizes_v2.txt\", \"r\") as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "parsed_data = [parse_line(line) for line in data][1:]\n",
    "\n",
    "conv_layers, total_data = filter_conv_layers(data, input_tensor.squeeze().shape)\n",
    "\n",
    "conv_layers[0:5], total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1029570560"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data transmission\n",
    "total_data *= 2\n",
    "total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5147852800\n",
      "10295705600 bytes\n",
      "9818.75 MB\n",
      "9.59 GB\n"
     ]
    }
   ],
   "source": [
    "expansion_factor = 5\n",
    "total_data *= expansion_factor\n",
    "print(total_data)\n",
    "\n",
    "bytes_per_value = 2  # For 16-bit precision\n",
    "total_data_bytes = total_data * bytes_per_value\n",
    "\n",
    "# Convert to MB and GB\n",
    "total_data_mb = total_data_bytes / (1024**2)\n",
    "total_data_gb = total_data_bytes / (1024**3)\n",
    "\n",
    "print(f\"{total_data_bytes} bytes\")\n",
    "print(f\"{total_data_mb:.2f} MB\")\n",
    "print(f\"{total_data_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "execution": {
   "timeout": 10800
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
