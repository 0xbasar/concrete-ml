{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0052f074",
   "metadata": {},
   "source": [
    "# Testing Titanic with Decision Trees\n",
    "\n",
    "This notebook was made for the [kaggle Titanic competition](https://www.kaggle.com/c/titanic/).\n",
    "\n",
    "With inspiration from the [ppxgboost repository](https://github.com/awslabs/privacy-preserving-xgboost-inference/blob/master/example/Titanic.ipynb), which is \"Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved. SPDX-License-Identifier: Apache-2.0\".\n",
    "\n",
    "It also took some ideas from several upvoted public notebooks, including [this one](https://www.kaggle.com/code/startupsci/titanic-data-science-solutions/notebook) from Manav Sehgal and [this one](https://www.kaggle.com/code/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy#Step-3:-Prepare-Data-for-Consumption) from LD Freeman."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e2ca58",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c415ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from concrete.ml.sklearn import XGBClassifier as ConcreteXGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b79c0e8",
   "metadata": {},
   "source": [
    "## Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5121bf3e",
   "metadata": {},
   "source": [
    "Be sure to run `make download_datasets` to have local versions of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5f3802c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the datasets\n",
    "if not os.path.isfile(\"./local_datasets/titanic/train.csv\"):\n",
    "    raise ValueError(\"Please launch `make download_datasets` to get datasets\")\n",
    "\n",
    "train_data = pd.read_csv(\"./local_datasets/titanic/train.csv\")\n",
    "test_data = pd.read_csv(\"./local_datasets/titanic/test.csv\")\n",
    "datasets = [train_data, test_data]\n",
    "\n",
    "# Save the passenger IDs used for submission\n",
    "test_ids = test_data[\"PassengerId\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ec9783",
   "metadata": {},
   "source": [
    "Let's take a closer look at the train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae653f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c26d9ae",
   "metadata": {},
   "source": [
    "We can observe:\n",
    "- the target variable: Survived\n",
    "- some numerical variables: PassengerID, Pclass, SbSp, Parch, Fare\n",
    "- some categorical (non-numerical) variables: Name, Sex, Ticket, Cabin, Embarked\n",
    "\n",
    "First, it seems that PassengerId and Ticket are supposed to be random Ids that should not impact the predictions so we can already remove them from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0db09e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_column = [\"PassengerId\", \"Ticket\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7259b1ab",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "\n",
    "Then, we can notice that some values are missing for the Cabin variable. We must therefore investigate a bit more about this by printing the total amounts of missing values for each variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "737d6fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Train data ---\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64 \n",
      "\n",
      "--- Test data ---\n",
      "PassengerId      0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age             86\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             1\n",
      "Cabin          327\n",
      "Embarked         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"-\" * 3, \"Train data\", \"-\" * 3)\n",
    "print(train_data.isnull().sum(), \"\\n\")\n",
    "print(\"-\" * 3, \"Test data\", \"-\" * 3)\n",
    "print(test_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f945a8f5",
   "metadata": {},
   "source": [
    "Only four variables are incomplete: Cabin, Age, Embarked and Fare. However, Cabin seems to be missing quite a lot of them compared to the others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0266c622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of missing values in Cabin: 77.5%\n"
     ]
    }
   ],
   "source": [
    "n_cabin_missing_values = pd.concat(datasets).Cabin.isnull().sum()\n",
    "n_total_values = pd.concat(datasets).shape[0]\n",
    "print(f\"Percentage of missing values in Cabin: {n_cabin_missing_values/n_total_values*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51794a8",
   "metadata": {},
   "source": [
    "Since the Cabin variables misses more than 2/3 of its values, it might not be relevant to keep it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35d9ceb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_column += [\"Cabin\"]\n",
    "\n",
    "for dataset in datasets:\n",
    "    # Remove unrelevant variables\n",
    "    dataset.drop(drop_column, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c3e038",
   "metadata": {},
   "source": [
    "For the other ones, we can replace the missing values using:\n",
    "- the median value for Age and Fare\n",
    "- the most common value for Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "565ec656",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    # Complete missing Age values with median\n",
    "    dataset.Age.fillna(dataset.Age.median(), inplace=True)\n",
    "\n",
    "    # Complete missing Embarked values with the most common one\n",
    "    dataset.Embarked.fillna(dataset.Embarked.mode()[0], inplace=True)\n",
    "\n",
    "    # Complete missing Fare values with median\n",
    "    dataset.Fare.fillna(dataset.Fare.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c84a2b",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "We can manually extract and create new features in order to help the model interpret some behaviors and correctly predict an outcome. Those new features are:\n",
    "- FamilySize: the size of the family the individual was travelling with, with 1 being someone travelling alone. \n",
    "- IsAlone: 1 if the individual was travelling alone, 0 if not. It might help the model emphasize on this idea of travelling with relatives or not.\n",
    "- Title: the individual's title (Mr, Mrs, ...), which might help the model to have a better idea of each one's social status.\n",
    "- Farebin and AgeBin: Binned version of the Fare and Age variables. Grouping values together might reduce the effects of minor observation errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff3e032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bin_labels(bin_name, number_of_bins):\n",
    "    labels = []\n",
    "    for i in range(number_of_bins):\n",
    "        labels.append(bin_name + f\"_{i}\")\n",
    "    return labels\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    # Emphasize on relatives\n",
    "    dataset[\"FamilySize\"] = dataset.SibSp + dataset.Parch + 1\n",
    "\n",
    "    dataset[\"IsAlone\"] = 1\n",
    "    dataset.IsAlone[dataset.FamilySize > 1] = 0\n",
    "\n",
    "    # Consider an individual's social status\n",
    "    dataset[\"Title\"] = dataset.Name.str.extract(r\" ([A-Za-z]+)\\.\", expand=False)\n",
    "\n",
    "    # Group fares and ages in \"bins\" or \"buckets\"\n",
    "    dataset[\"FareBin\"] = pd.qcut(dataset.Fare, 4, labels=get_bin_labels(\"FareBin\", 4))\n",
    "    dataset[\"AgeBin\"] = pd.cut(dataset.Age.astype(int), 5, labels=get_bin_labels(\"AgeBin\", 5))\n",
    "\n",
    "    # Remove now-unrelevant variables\n",
    "    drop_column = [\"Name\", \"SibSp\", \"Parch\", \"Fare\", \"Age\"]\n",
    "    dataset.drop(drop_column, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b5576e",
   "metadata": {},
   "source": [
    "Let's have a look on the titles' distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "638f3f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr          757\n",
      "Miss        260\n",
      "Mrs         197\n",
      "Master       61\n",
      "Rev           8\n",
      "Dr            8\n",
      "Col           4\n",
      "Mlle          2\n",
      "Major         2\n",
      "Ms            2\n",
      "Lady          1\n",
      "Sir           1\n",
      "Mme           1\n",
      "Don           1\n",
      "Capt          1\n",
      "Countess      1\n",
      "Jonkheer      1\n",
      "Dona          1\n",
      "Name: Title, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = pd.concat(datasets)\n",
    "titles = data.Title.value_counts()\n",
    "print(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484bfd4",
   "metadata": {},
   "source": [
    "We can clearly observe that only a few titles represent most of the individuals. In order to prevent the model from becoming overly specific, we decide to group all the \"uncommon\" titles together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "498273da",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncommon_titles = titles[titles < 10].index\n",
    "\n",
    "for dataset in datasets:\n",
    "    dataset.Title.replace(uncommon_titles, \"Rare\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18937e31",
   "metadata": {},
   "source": [
    "### Dummification\n",
    "\n",
    "Finally, we can \"dummify\" the remaining categorical variables. Dummification is a common technique of transforming categorical (non-numerical) data into numerical data without having to map values or consider any order between each of them. The idea is to take all the different values found in a variable and create a new binary variable associated to each one of them. \n",
    "\n",
    "For example, the \"Embarked\" variable has three categorical values: \"S\", \"C\" and \"Q\". Dummifying the data will create three new variables, \"Embarked_S\", \"Embarked_C\" and \"Embarked_Q\", and set the value of \"Embarked_S\" (resp. \"Embarked_C\" and \"Embarked_Q\") to 1 for each data point initially labeled with \"S\" (resp. \"C\" and \"Q\") in the \"Embarked\" variable, else 0.\n",
    "\n",
    "We also extract the input and target values used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b0539c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Survived\"\n",
    "categorical_features = train_data.select_dtypes(exclude=np.number).columns\n",
    "\n",
    "x_train = pd.get_dummies(train_data, prefix=categorical_features)\n",
    "x_train = x_train.drop(columns=[target])\n",
    "\n",
    "x_test = pd.get_dummies(test_data, prefix=categorical_features)\n",
    "\n",
    "y_train = train_data[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c109f4",
   "metadata": {},
   "source": [
    "Computing the predictions in FHE on a test set of about 400 data points can take up to several hours. We therefore reduce the test size in order to be able to run the notebook much faster. However, it is necessary to remove the following line for proper submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c39396e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove this line for proper submission\n",
    "x_test = x_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6b246c",
   "metadata": {},
   "source": [
    "## Training \n",
    "### Training with XGBoost\n",
    "\n",
    "Let's train a model using XGBoost. Since several parameters have to be fixed beforehand, we use scikit-learn's GridSearchCV method to perform cross validation in order to maximise our chance to find the best ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cc269e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Cross-Validation generator\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n",
    "\n",
    "# Set the parameters to tune\n",
    "param_grid = {\n",
    "    \"max_depth\": [5, 10, 15],\n",
    "    \"n_estimators\": [5, 10, 100],\n",
    "    \"learning_rate\": [0.01, 0.1, 1],\n",
    "}\n",
    "\n",
    "# Instantiate and fit the model\n",
    "model = GridSearchCV(XGBClassifier(), param_grid, cv=cv, scoring=\"roc_auc\")\n",
    "model.fit(x_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfde39b3",
   "metadata": {},
   "source": [
    "### Training with Concrete-ML\n",
    "\n",
    "Now, let's do the same with Concrete-ML's XGBClassifier method. \n",
    "\n",
    "In order to do so, we need to specify the number of bits over which inputs, outputs and weights will be quantized. This value can influence the precision of the model as well as its inference running time. In our case, setting this value to 2 bits outputs a satisfying accuracy score while running faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cc1cc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Concrete-ML model needs an additional parameter used for quantization\n",
    "param_grid[\"n_bits\"] = [2]\n",
    "\n",
    "x_train = x_train.astype(np.float32)\n",
    "\n",
    "# Instantiate and fit the model\n",
    "concrete_model = GridSearchCV(ConcreteXGBClassifier(), param_grid, cv=cv, scoring=\"roc_auc\")\n",
    "concrete_model.fit(x_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40aba09",
   "metadata": {},
   "source": [
    "## Predicting the Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5675c1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the predictions in clear using XGBoost\n",
    "clear_predictions = model.predict(x_test)\n",
    "\n",
    "# Computing the predictions in clear using Concrete-ML\n",
    "clear_quantized_predictions = concrete_model.predict(x_test)\n",
    "\n",
    "# Compiling the Concrete-ML model on a subset\n",
    "fhe_circuit = concrete_model.best_estimator_.compile(x_train.head(100).to_numpy())\n",
    "\n",
    "# Generating the keys\n",
    "# This step is not necessary, keygen() is called directly within the predict method. However, since\n",
    "# the keys are stored in cache by default, it is useful to run it beforehand in order to be able to\n",
    "# measure the prediction executing time separately from the key generation one\n",
    "time_begin = time.time()\n",
    "fhe_circuit.keygen()\n",
    "key_generation_duration = time.time() - time_begin\n",
    "\n",
    "# Computing the predictions in FHE using Concrete-ML\n",
    "time_begin = time.time()\n",
    "fhe_predictions = concrete_model.best_estimator_.predict(x_test, execute_in_fhe=True)\n",
    "prediction_duration = time.time() - time_begin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95331f92",
   "metadata": {},
   "source": [
    "### Compare the predictions\n",
    "\n",
    "Let's see how different the predictions are between the XGBoost model and the Concrete-ML ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8874f54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction similarity between Concrete-ML (clear) and XGBoost (clear) models: 100.00%\n"
     ]
    }
   ],
   "source": [
    "nb_equal_preds = np.sum(clear_quantized_predictions == clear_predictions)\n",
    "clear_prediction_similarity = nb_equal_preds / len(clear_predictions) * 100\n",
    "print(\n",
    "    \"Prediction similarity between Concrete-ML (clear) and XGBoost (clear) models: \"\n",
    "    f\"{clear_prediction_similarity:.2f}%\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47ef02e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction similarity between Concrete-ML (FHE) and XGBoost (clear) models: 100.00%\n",
      "Key generation time: 22.5014s\n",
      "Execution time per inference: 396.0776 s\n"
     ]
    }
   ],
   "source": [
    "nb_equal_preds = np.sum(fhe_predictions == clear_predictions)\n",
    "fhe_prediction_similarity = nb_equal_preds / len(clear_predictions) * 100\n",
    "print(\n",
    "    \"Prediction similarity between Concrete-ML (FHE) and XGBoost (clear) models: \"\n",
    "    f\"{fhe_prediction_similarity:.2f}%\",\n",
    ")\n",
    "\n",
    "print(f\"Key generation time: {key_generation_duration:.04f}s\")\n",
    "print(f\"Execution time per inference: {prediction_duration / len(clear_predictions):.04f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050e58bf",
   "metadata": {},
   "source": [
    "### Save the Kaggle Submission File\n",
    "\n",
    "When submitted to the Kaggle platform, this FHE model outputs an accuracy of around 77%.\n",
    "\n",
    "In fact, using the given dataset, most of the submitted notebooks do not seem to exceed 79% of accuracy. Therefore, additional pre-processing and feature engineering might help increasing our current score but probably not by much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9c16d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predictions should not be saved if the test data was reduced for faster runs.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    submission = pd.DataFrame(\n",
    "        {\n",
    "            \"PassengerId\": test_ids,\n",
    "            \"Survived\": fhe_predictions,\n",
    "        }\n",
    "    )\n",
    "    submission.to_csv(\"titanic_submission.csv\", index=False)\n",
    "except ValueError:\n",
    "    print(\"The predictions should not be saved if the test data was reduced for faster runs.\")"
   ]
  }
 ],
 "metadata": {
  "execution": {
   "timeout": 10800
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
