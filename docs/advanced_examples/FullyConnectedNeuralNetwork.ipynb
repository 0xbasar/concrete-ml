{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Neural Network\n",
    "\n",
    "In this example, we show how one can train a neural network on a specific task (here, Iris Classification) and use Concrete-Numpy to make the model work in FHE settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCIris(torch.nn.Module):\n",
    "    \"\"\"Neural network for Iris classification\n",
    "\n",
    "    We define a fully connected network with three (3) fully connected (fc) layers that\n",
    "    perform feature extraction and one (fc) layer to produce the final classification.\n",
    "    We will use 3 neurons on all layers to ensure that the FHE accumulators\n",
    "    do not overflow (we are currently only allowed a maximum of 8 bits-width).\n",
    "    More information on this is available at\n",
    "    https://docs.zama.ai/concrete-numpy/how-to/reduce_needed_precision.html#limitations-for-fhe-friendly-neural-network.\n",
    "\n",
    "    Due to accumulator limits, we have to design a network with only a few neurons on each layer.\n",
    "    This is in contrast to a traditional approach where the number of neurons increases after\n",
    "    each layer or block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # The first layer processes the input data, in our case 4 dimensional vectors\n",
    "        self.linear1 = nn.Linear(input_size, 3)\n",
    "        self.sigmoid1 = nn.Sigmoid()\n",
    "        # Next, we add a one intermediate layer\n",
    "        self.linear2 = nn.Linear(3, 3)\n",
    "        self.sigmoid2 = nn.Sigmoid()\n",
    "        # Finally, we add the decision layer for 3 output classes encoded as one-hot vectors\n",
    "        self.decision = nn.Linear(3, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.sigmoid1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.sigmoid2(x)\n",
    "        x = self.decision(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define all required variables to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Split into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train = torch.tensor(X_train).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_train = torch.tensor(y_train)\n",
    "y_test = torch.tensor(y_test)\n",
    "\n",
    "# Initialize our model\n",
    "model = FCIris(X.shape[1])\n",
    "\n",
    "# Define our loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define our optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Define the number of iterations\n",
    "n_iters = 50001\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    for i in range(n_iters):\n",
    "        # Get a random batch of training data\n",
    "        idx = torch.randperm(X_train.size()[0])\n",
    "        X_batch = X_train[idx][:batch_size]\n",
    "        y_batch = y_train[idx][:batch_size]\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(X_batch)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            # Print epoch number, loss and accuracy\n",
    "            accuracy = torch.sum(torch.argmax(y_pred, dim=1) == y_batch).item() / y_batch.size()[0]\n",
    "            print(f\"Iterations: {i:02} | Loss: {loss.item():.4f} | Accuracy: {100*accuracy:.2f}%\")\n",
    "            if accuracy == 1:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model\n",
    "\n",
    "The `compile_torch_model` first applies a quantization to `model` with `n_bits` of precision using `X_train` as the calibration dataset and compile the model to its FHE counterparts. Here we use 3 bits of precision. In some edge cases, the network accumulators can overflow (i.e. extreme quantized values in both input and weights, which is unlikely). In such a case, we need to retrain the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a FHE friendly quantized network.\n",
      "Iterations: 00 | Loss: 1.0013 | Accuracy: 50.00%\n",
      "Iterations: 1000 | Loss: 0.4915 | Accuracy: 100.00%\n",
      "Compiling the model to FHE.\n",
      "The network is trained and FHE friendly.\n"
     ]
    }
   ],
   "source": [
    "from concrete.ml.torch.compile import compile_torch_model\n",
    "\n",
    "print(\"Training a FHE friendly quantized network.\")\n",
    "for _ in range(10):\n",
    "    try:\n",
    "        train()\n",
    "        print(\"Compiling the model to FHE.\")\n",
    "        quantized_compiled_module = compile_torch_model(\n",
    "            model,\n",
    "            X_train,\n",
    "            n_bits={\"net_inputs\": 7, \"op_inputs\": 3, \"op_weights\": 4, \"net_outputs\": 8},\n",
    "        )\n",
    "        print(\"The network is trained and FHE friendly.\")\n",
    "        break\n",
    "    except RuntimeError as e:\n",
    "        if str(e).startswith(\"max_bit_width of some nodes is too high\"):\n",
    "            print(\"The network is not fully FHE friendly, retraining.\")\n",
    "            continue\n",
    "        raise e\n",
    "else:\n",
    "    raise RuntimeError(\n",
    "        \"Could not compile the model to FHE.\"\n",
    "        \"You may need to decrease the n_bits parameter to avoid potential overflows.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with the torch model in clear and with the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now have a module in full numpy.\n",
    "# Convert data to a numpy array.\n",
    "X_train_numpy = X_train.numpy()\n",
    "X_test_numpy = X_test.numpy()\n",
    "y_train_numpy = y_train.numpy()\n",
    "y_test_numpy = y_test.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(X_test)\n",
    "q_X_test_numpy = quantized_compiled_module.quantize_input(X_test_numpy)\n",
    "quant_model_predictions = quantized_compiled_module(q_X_test_numpy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a key for an {quantized_compiled_module.fhe_circuit.graph.maximum_integer_bit_width()}-bit circuit\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Generating a key for an \"\n",
    "    f\"{quantized_compiled_module.fhe_circuit.graph.maximum_integer_bit_width()}-bit circuit\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_begin = time.time()\n",
    "quantized_compiled_module.fhe_circuit.client.keygen(force=False)\n",
    "print(f\"Key generation time: {time.time() - time_begin} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict in FHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "fhe_x_test = quantized_compiled_module.quantize_input(X_test_numpy)\n",
    "homomorphic_quant_predictions = []\n",
    "time_begin = time.time()\n",
    "for x_q in tqdm(fhe_x_test):\n",
    "    homomorphic_quant_predictions.append(\n",
    "        quantized_compiled_module.forward_fhe.encrypt_run_decrypt(np.array([x_q]).astype(np.uint8))\n",
    "    )\n",
    "homomorphic_predictions = quantized_compiled_module.dequantize_output(\n",
    "    np.array(homomorphic_quant_predictions, dtype=np.int64).reshape(quant_model_predictions.shape)\n",
    ")\n",
    "\n",
    "print(f\"Execution time: {(time.time() - time_begin) / len(fhe_x_test)} seconds per sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the accuracy of both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_0 = 100 * (y_pred.argmax(1) == y_test).float().mean()\n",
    "acc_1 = 100 * (quant_model_predictions.argmax(1) == y_test_numpy).mean()\n",
    "acc_2 = 100 * (homomorphic_predictions.argmax(1) == y_test_numpy).mean()\n",
    "\n",
    "print(f\"Test Accuracy: {acc_0:.2f}%\")\n",
    "print(f\"Test Accuracy Quantized Inference: {acc_1:.2f}%\")\n",
    "print(f\"Test Accuracy Homomorphic Inference: {acc_2:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_train_2d = pca.fit_transform(X_train_numpy)\n",
    "\n",
    "b_min = np.min(X_train_2d, axis=0)\n",
    "b_max = np.max(X_train_2d, axis=0)\n",
    "\n",
    "grid_dims = tuple(np.linspace(b_min[i], b_max[i], 512) for i in range(X_train_2d.shape[1]))\n",
    "ndgrid_tuple = np.meshgrid(*grid_dims)\n",
    "grid_2d = np.vstack([g.ravel() for g in ndgrid_tuple]).transpose()\n",
    "\n",
    "grid_test = pca.inverse_transform(grid_2d)\n",
    "grid_test_quantized = quantized_compiled_module.quantize_input(grid_test)\n",
    "\n",
    "grid_pred_all = quantized_compiled_module(grid_test_quantized)\n",
    "grid_pred_all_original = model(torch.tensor(grid_test).float()).detach().numpy()\n",
    "\n",
    "pred_classes = np.argmax(grid_pred_all, axis=1).astype(np.int32)\n",
    "pred_classes_original = np.argmax(grid_pred_all_original, axis=1).astype(np.int32)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "cmap = \"autumn\"\n",
    "\n",
    "# Create two subplots and set their locations\n",
    "plt.clf()\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot original model contour plot\n",
    "axs[0].contourf(\n",
    "    ndgrid_tuple[0],\n",
    "    ndgrid_tuple[1],\n",
    "    pred_classes_original.reshape(ndgrid_tuple[0].shape),\n",
    "    cmap=cmap,\n",
    ")\n",
    "\n",
    "# Plot the scatter with marker borders\n",
    "axs[0].scatter(X_train_2d[:, 0], X_train_2d[:, 1], c=y_train_numpy, s=50, edgecolors=\"k\", cmap=cmap)\n",
    "\n",
    "# Add title and axis labels\n",
    "axs[0].set_title(\"Original Inference\")\n",
    "\n",
    "\n",
    "# Plot quantized model contour plot\n",
    "axs[1].contourf(\n",
    "    ndgrid_tuple[0], ndgrid_tuple[1], pred_classes.reshape(ndgrid_tuple[0].shape), cmap=cmap\n",
    ")\n",
    "\n",
    "# Plot the scatter with marker borders\n",
    "axs[1].scatter(X_train_2d[:, 0], X_train_2d[:, 1], c=y_train_numpy, s=50, edgecolors=\"k\", cmap=cmap)\n",
    "\n",
    "# Add title and axis labels\n",
    "axs[1].set_title(\"Quantized Inference\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot, we show the decision boundaries for both the original and quantized model. The quantized model has its decision boundaries (colored regions) slightly shifted compared to the original model. This is due to the low-bit quantization applied to the model in post-training.\n",
    "\n",
    "Here we do not compute the contour plot for the FHE inference as this would be really costly but it should be pretty close to the quantized model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we presented a few steps to have a model (torch neural network) inference in over homomorphically encrypted data: \n",
    "- We first trained a fully connected neural network yielding ~97% accuracy\n",
    "- Then, we quantized it using Concrete-Numpy. We observed that accuracy is preserved when quantizing to 3 bits weights and activations.\n",
    "- We then compiled the model to its FHE equivalent and ran inference to get our FHE predictions over the test set\n",
    "\n",
    "The homomorphic inference achieves the same accuracy as the quantized model inference.\n",
    "\n",
    "### Note on reproducibility\n",
    "\n",
    "Post-training quantization with such a low bit width (<=3) can yield different results for the quantized model depending on the values of the learned weights."
   ]
  }
 ],
 "metadata": {
  "execution": {
   "timeout": 10800
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
