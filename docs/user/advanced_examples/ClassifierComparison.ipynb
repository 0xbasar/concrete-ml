{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49db8ae4",
   "metadata": {},
   "source": [
    "# Classifier Comparison\n",
    "\n",
    "In this example we show how all **Concrete-ML** classifiers can be used in a manner that is very \n",
    "similar to scikit-learn classifiers. FHE classifiers have an API that builds upon the scikit-learn \n",
    "API, adding two new elements:\n",
    "\n",
    "* compiling the model to FHE\n",
    "* predicting in FHE\n",
    "\n",
    "The FHE classifiers, which quantize the inputs and model parameters, can be tested in a simulated\n",
    "FHE environment called the \"Virtual Library\". This type of execution is much faster than running in\n",
    "FHE, but it does not operate over encrypted data. However, the Virtual Library is very useful to \n",
    "design and train FHE compatible classifiers, as it allows the user to investigate whether the FHE\n",
    "constraints are met at design time.\n",
    "\n",
    "### FHE runtime considerations and simulation\n",
    "\n",
    "In this demo the test data is classified in FHE and the decision function values for the domain\n",
    "grid are computed using simulation, using the Virtual Library. Thus:\n",
    "\n",
    "* the accuracy reported is computed in FHE\n",
    "* the red/blue decision function contours are computed with simulation\n",
    "\n",
    "However, since the runtimes in FHE are high (it can take up to an hour to run test set \n",
    "classification in FHE), you have the option to run everything in simulation by \n",
    "setting ALWAYS_USE_VL=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "561acf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source:\n",
    "#   https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
    "\n",
    "# Code source: Gaël Varoquaux\n",
    "#              Andreas Müller\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# Modified to integrate Concrete-ML functions by Zama\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from concrete.numpy.compilation.configuration import CompilationConfiguration\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.datasets import make_circles, make_classification, make_moons\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from skorch.classifier import NeuralNetClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "from concrete.ml.sklearn import DecisionTreeClassifier as ConcreteDecisionTreeClassifier\n",
    "from concrete.ml.sklearn import LinearSVC as ConcreteLinearSVC\n",
    "from concrete.ml.sklearn import LogisticRegression as ConcreteLogisticRegression\n",
    "from concrete.ml.sklearn import NeuralNetClassifier as ConcreteNeuralNetClassifier\n",
    "from concrete.ml.sklearn import RandomForestClassifier as ConcreteRandomForestClassifier\n",
    "from concrete.ml.sklearn import XGBClassifier as ConcreteXGBClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "concrete_classifiers = [\n",
    "    ConcreteDecisionTreeClassifier,\n",
    "    ConcreteLinearSVC,\n",
    "    ConcreteLogisticRegression,\n",
    "    ConcreteNeuralNetClassifier,\n",
    "]\n",
    "\n",
    "ALWAYS_USE_VL = False\n",
    "\n",
    "# We predict in FHE on the datasets and using the Virtual Library on the domain grid\n",
    "# Note that the Virtual Library executes an integer model on data in the clear\n",
    "# Nothing is encrypted when running in the Virtual Library\n",
    "COMPIL_CONFIG_VL = CompilationConfiguration(\n",
    "    dump_artifacts_on_unexpected_failures=False,\n",
    "    enable_unsafe_features=True,  # This is for our tests in Virtual Library only\n",
    "    # BCM treat_warnings_as_errors=False,\n",
    "    use_insecure_key_cache=True,  # This is for our tests only, never use that in prod\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ae409e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pylint: disable=too-many-locals,too-many-statements\n",
    "def make_classifier_comparison(title, classifier_list):\n",
    "\n",
    "    h = 0.04  # Step size in the mesh\n",
    "\n",
    "    X, y = make_classification(\n",
    "        n_samples=200,\n",
    "        n_features=2,\n",
    "        n_redundant=0,\n",
    "        n_informative=2,\n",
    "        random_state=1,\n",
    "        n_clusters_per_class=1,\n",
    "    )\n",
    "    rng = np.random.RandomState(2)\n",
    "    X += 2 * rng.uniform(size=X.shape)\n",
    "    linearly_separable = (X, y)\n",
    "\n",
    "    datasets = [\n",
    "        make_moons(n_samples=200, noise=0.3, random_state=0),\n",
    "        make_circles(n_samples=200, noise=0.2, factor=0.5, random_state=1),\n",
    "        linearly_separable,\n",
    "    ]\n",
    "\n",
    "    font_size_text = 24\n",
    "\n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "    fig, axs = plt.subplots(len(datasets), len(classifier_list) + 1, figsize=(32, 16))\n",
    "    fig.suptitle(title, fontsize=20)\n",
    "    fig.patch.set_facecolor(\"white\")\n",
    "\n",
    "    # Iterate over datasets\n",
    "    for ds_cnt, ds in enumerate(datasets):\n",
    "        # Preprocess dataset, split into training and test part\n",
    "        X, y = ds\n",
    "        X = X.astype(np.float32)\n",
    "        X = StandardScaler().fit_transform(X)\n",
    "\n",
    "        # We use 15 percent (30 points for a dataset of 200 points) for prediction\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "        x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "        y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "        # Just plot the dataset first\n",
    "        cm = plt.cm.RdBu\n",
    "        cm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\n",
    "        ax = axs[i, 0]\n",
    "        if ds_cnt == 0:\n",
    "            ax.set_title(\"Input data\", fontsize=font_size_text)\n",
    "\n",
    "        # Plot the training points\n",
    "        ax.scatter(\n",
    "            X_train[:, 0],\n",
    "            X_train[:, 1],\n",
    "            c=y_train,\n",
    "            cmap=cm_bright,\n",
    "            edgecolors=\"k\",\n",
    "            label=\"Train data\",\n",
    "        )\n",
    "\n",
    "        # Plot the testing points\n",
    "        ax.scatter(\n",
    "            X_test[:, 0],\n",
    "            X_test[:, 1],\n",
    "            marker=\"D\",\n",
    "            c=y_test,\n",
    "            cmap=cm_bright,\n",
    "            alpha=0.6,\n",
    "            edgecolors=\"k\",\n",
    "            label=\"Test data\",\n",
    "        )\n",
    "        ax.legend()\n",
    "\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        j = 1\n",
    "\n",
    "        # Iterate over classifiers\n",
    "        for clf_klass, name in classifier_list:\n",
    "\n",
    "            ax = axs[i, j]\n",
    "\n",
    "            clf = clf_klass()\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "            # Check if this is a Concrete-ML classifier that needs compilation to FHE\n",
    "            is_an_fhe_classifier = clf.__class__ in concrete_classifiers\n",
    "            if is_an_fhe_classifier:\n",
    "                # First we compile the classifier in FHE\n",
    "                clf.compile(\n",
    "                    X_train,\n",
    "                    use_virtual_lib=ALWAYS_USE_VL,\n",
    "                    configuration=COMPIL_CONFIG_VL if ALWAYS_USE_VL else None,\n",
    "                )\n",
    "                # And we predict on the dataset in FHE\n",
    "                y_pred = clf.predict(X_test, execute_in_fhe=True)\n",
    "            else:\n",
    "                # We predict in float32 for sklearn classifiers\n",
    "                y_pred = clf.predict(X_test)\n",
    "\n",
    "            # Measure accuracy\n",
    "            score = accuracy_score(y_pred, y_test)\n",
    "\n",
    "            # We plot the decision function using the predictions made using the Virtual Library\n",
    "            # This feature allows to simulate FHE execution without paying the cost of FHE\n",
    "            # computation. But this is not actually running predictions on encrypted data,\n",
    "            # data is not encrypted when using the Virtual Library\n",
    "            if is_an_fhe_classifier:\n",
    "                clf.compile(\n",
    "                    X_train,\n",
    "                    use_virtual_lib=True,\n",
    "                    configuration=COMPIL_CONFIG_VL,\n",
    "                )\n",
    "\n",
    "            # Plot the decision boundary. For that, we will assign a color to each\n",
    "            # point in the mesh, which is obtained as a cartesian\n",
    "            # product of [x_min, x_max] with [y_min, y_max].\n",
    "            if hasattr(clf, \"decision_function\"):\n",
    "                if is_an_fhe_classifier:\n",
    "                    # Note that although we pass execute_in_fhe=True, execution is only simulated\n",
    "                    # since we compiled with use_virtual_lib=True\n",
    "                    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()], execute_in_fhe=True)\n",
    "                else:\n",
    "                    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "            else:\n",
    "                if is_an_fhe_classifier:\n",
    "                    # Note that although we pass execute_in_fhe=True, execution is only simulated\n",
    "                    # since we compiled with use_virtual_lib=True\n",
    "                    Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()], execute_in_fhe=True)[:, 1]\n",
    "                else:\n",
    "                    Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "            # Put the result into a color plot\n",
    "            Z = Z.reshape(xx.shape)\n",
    "            ax.contourf(xx, yy, Z, cmap=cm, alpha=0.8)\n",
    "\n",
    "            # Plot the training points\n",
    "            ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\")\n",
    "\n",
    "            # Plot the testing points\n",
    "            ax.scatter(\n",
    "                X_test[:, 0],\n",
    "                X_test[:, 1],\n",
    "                c=y_test,\n",
    "                marker=\"D\",\n",
    "                cmap=cm_bright,\n",
    "                edgecolors=\"k\",\n",
    "                alpha=0.6,\n",
    "            )\n",
    "\n",
    "            ax.contour(\n",
    "                xx,\n",
    "                yy,\n",
    "                Z,\n",
    "                levels=[0.5],\n",
    "                linewidths=2,\n",
    "            )\n",
    "\n",
    "            ax.set_xlim(xx.min(), xx.max())\n",
    "            ax.set_ylim(yy.min(), yy.max())\n",
    "            ax.set_xticks(())\n",
    "            ax.set_yticks(())\n",
    "            if ds_cnt == 0:\n",
    "                ax.set_title(name, fontsize=font_size_text)\n",
    "            ax.text(\n",
    "                xx.max() - 0.3,\n",
    "                yy.min() + 0.3,\n",
    "                f\"{score*100:0.1f}%\",\n",
    "                size=font_size_text,\n",
    "                horizontalalignment=\"right\",\n",
    "            )\n",
    "            j += 1\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368811e0",
   "metadata": {},
   "source": [
    "### Neural-net-based classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2268ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_neural_net = {\n",
    "    \"module__n_outputs\": 2,\n",
    "    \"module__input_dim\": 2,\n",
    "    \"module__n_w_bits\": 2,\n",
    "    \"module__n_a_bits\": 2,\n",
    "    \"module__n_accum_bits\": 32,\n",
    "    \"module__n_hidden_neurons_multiplier\": 1,\n",
    "    \"module__n_layers\": 1,\n",
    "    \"module__activation_function\": torch.nn.ReLU,\n",
    "    \"max_epochs\": 200,\n",
    "    \"verbose\": 0,\n",
    "    \"lr\": 0.001,\n",
    "}\n",
    "\n",
    "\n",
    "class FC(torch.nn.Module):\n",
    "    def __init__(self, activation_function):\n",
    "        \"\"\"Setup the fully connected neural network.\n",
    "\n",
    "        Please note that the number of neurons on each intermediate layer is N=12. A\n",
    "        linear layer will compute, for an output neuron k, the dot product of weights and inputs,\n",
    "\n",
    "             output_k = <w,x> = Sum_N(w_ki * x_i).\n",
    "\n",
    "        The maximum bit width for output_k is 8 bits and we will quantize w_ki and x_i to 2 bits.\n",
    "        Therefore, the maximum number of neurons on the intermediary layers, to be 100% FHE\n",
    "        compatible, is N = 14. We use a slightly lower value to decrease computation time.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(in_features=2, out_features=12)\n",
    "        self.fc3 = torch.nn.Linear(in_features=12, out_features=12)\n",
    "        self.fc4 = torch.nn.Linear(in_features=12, out_features=2)\n",
    "        self.act = activation_function()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        out = self.fc1(x.to(torch.float32))\n",
    "        out = self.act(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.act(out)\n",
    "        out = self.fc4(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "nn_classifier_list = [\n",
    "    (partial(MLPClassifier, alpha=1, max_iter=1000), \"Sklearn MLP\"),\n",
    "    (\n",
    "        partial(\n",
    "            NeuralNetClassifier,\n",
    "            FC(torch.nn.ReLU),\n",
    "            verbose=0,\n",
    "            criterion=torch.nn.CrossEntropyLoss,\n",
    "            optimizer=torch.optim.Adam,\n",
    "            lr=0.001,\n",
    "            max_epochs=200,\n",
    "            batch_size=32,\n",
    "        ),\n",
    "        \"Torch Neural Net\",\n",
    "    ),\n",
    "    (\n",
    "        partial(ConcreteNeuralNetClassifier, batch_size=32, **params_neural_net),\n",
    "        \"Concrete Neural Net\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "make_classifier_comparison(\"NN Classifiers\", nn_classifier_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c562bdbe",
   "metadata": {},
   "source": [
    "### Linear classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2212bf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_classifier_list = [\n",
    "    # Clear classifiers\n",
    "    (partial(SVC, kernel=\"linear\", C=0.025), \"Linear SVC\"),\n",
    "    (LogisticRegression, \"Logistic Regression\"),\n",
    "    # FHE classifiers\n",
    "    (\n",
    "        partial(ConcreteLinearSVC, n_bits={\"inputs\": 5, \"weights\": 2}, C=0.025),\n",
    "        \"Concrete Linear SVC\",\n",
    "    ),\n",
    "    (\n",
    "        partial(ConcreteLogisticRegression, n_bits={\"inputs\": 5, \"weights\": 2}),\n",
    "        \"Concrete Logistic Regression\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "make_classifier_comparison(\"Linear Classifiers\", linear_classifier_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58abd7d7",
   "metadata": {},
   "source": [
    "### Tree and tree ensemble classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14711adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_classifier_list = [\n",
    "    (partial(DecisionTreeClassifier, max_depth=5), \"Decision Tree\"),\n",
    "    (partial(ConcreteDecisionTreeClassifier, max_depth=5), \"Concrete Decision Tree\"),\n",
    "    (\n",
    "        partial(RandomForestClassifier, max_depth=5, n_estimators=10, max_features=1),\n",
    "        \"Random Forest\",\n",
    "    ),\n",
    "    (ConcreteRandomForestClassifier, \"Concrete Random Forest\"),\n",
    "    (\n",
    "        partial(XGBClassifier, n_jobs=1, n_estimators=20, max_depth=3, eval_metric=\"logloss\"),\n",
    "        \"Sklearn XGB\",\n",
    "    ),\n",
    "    (partial(ConcreteXGBClassifier, n_jobs=1), \"Concrete XGB\"),\n",
    "]\n",
    "\n",
    "make_classifier_comparison(\"Tree-Based Classifiers\", tree_classifier_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aecb3bb",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this example we showed the results of three types of classifiers: \n",
    "\n",
    "* linear (Support Vector Machine, Logistic Regression)\n",
    "* neural-networks (multi-layer non-linear models)\n",
    "* tree-based (Decision Tree, Random Forest, XGBoost)\n",
    "\n",
    "The accuracy of the **Concrete** classifiers is measured on encrypted data. These classifiers \n",
    "work with parameters and inputs that are heavily quantized and, thus, show accuracy loss:\n",
    "\n",
    "* linear models: for this simple 2D case, linear models have good performance in FHE, similar to \n",
    "the performance of their fp32 counterparts\n",
    "* neural networks: since several layers increase the number of computations in these classifiers, \n",
    "the effect of precision loss is compounded. These classifiers have poor performance in FHE\n",
    "* tree-based classifiers: these classifiers achieve good accuracy both in fp32 and in quantized \n",
    "mode in FHE on encrypted data. Due to the particular computations in tree-based models, performance \n",
    "in FHE is maintained at the fp32 levels even on datasets which have a much higher number of \n",
    "dimensions\n",
    "\n",
    "## Future work\n",
    "\n",
    "In future releases of **Concrete-ML** we will improve the performance of linear classifiers on data\n",
    "with more dimensions and we will improve neural networks to handle heavy quantization while \n",
    "minimizing the loss of accuracy."
   ]
  }
 ],
 "metadata": {
  "execution": {
   "timeout": 10800
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
